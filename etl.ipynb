{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e21ee56d",
   "metadata": {},
   "source": [
    "# 🔄 Proceso ETL - Dataset Olist hacia MongoDB\n",
    "\n",
    "## Extracción, Transformación y Carga de datos de e-commerce brasileño\n",
    "\n",
    "Este notebook implementa un proceso ETL completo para:\n",
    "1. **Extraer** datos de archivos CSV del dataset Olist\n",
    "2. **Transformar** y limpiar los datos para análisis\n",
    "3. **Cargar** los datos procesados en MongoDB\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75af7fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Librerías importadas exitosamente\n",
      "📅 Proceso ETL iniciado: 2025-08-13 22:25:52\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "\n",
    "# Configuraciones\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Librerías importadas exitosamente\")\n",
    "print(f\"📅 Proceso ETL iniciado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec60fa9",
   "metadata": {},
   "source": [
    "### 📚 **Importación de Librerías**\n",
    "\n",
    "Esta celda configura todas las dependencias necesarias para el proceso ETL:\n",
    "\n",
    "- **pandas & numpy**: Manipulación y análisis de datos\n",
    "- **pymongo**: Cliente oficial de MongoDB para Python\n",
    "- **datetime**: Manejo de fechas y timestamps\n",
    "- **logging**: Sistema de registro de eventos y errores\n",
    "- **typing**: Anotaciones de tipos para mejor documentación del código\n",
    "\n",
    "**Configuraciones importantes:**\n",
    "- Se suprimen las advertencias para output más limpio\n",
    "- Se configura pandas para mostrar todas las columnas\n",
    "- Se establece logging con formato timestamp para trazabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb7c71",
   "metadata": {},
   "source": [
    "## 1. 📥 EXTRACCIÓN (Extract)\n",
    "\n",
    "Carga de datos desde archivos CSV del dataset Olist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56672175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:25:55,481 - INFO - 🔄 Iniciando extracción de datos...\n",
      "2025-08-13 22:25:55,646 - INFO - ✅ customers: 99,441 filas cargadas\n",
      "2025-08-13 22:25:56,272 - INFO - ✅ geolocation: 1,000,163 filas cargadas\n",
      "2025-08-13 22:25:56,530 - INFO - ✅ order_items: 112,650 filas cargadas\n",
      "2025-08-13 22:25:56,636 - INFO - ✅ order_payments: 103,886 filas cargadas\n",
      "2025-08-13 22:25:56,991 - INFO - ✅ order_reviews: 99,224 filas cargadas\n",
      "2025-08-13 22:25:57,438 - INFO - ✅ orders: 99,441 filas cargadas\n",
      "2025-08-13 22:25:57,480 - INFO - ✅ products: 32,951 filas cargadas\n",
      "2025-08-13 22:25:57,486 - INFO - ✅ sellers: 3,095 filas cargadas\n",
      "2025-08-13 22:25:57,488 - INFO - ✅ product_categories: 71 filas cargadas\n",
      "2025-08-13 22:25:57,489 - INFO - 📊 Extracción completada: 9 datasets, 1,550,922 filas totales\n"
     ]
    }
   ],
   "source": [
    "def extract_data() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Extrae todos los archivos CSV del dataset Olist\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Diccionario con todos los datasets cargados\n",
    "    \"\"\"\n",
    "    logger.info(\"🔄 Iniciando extracción de datos...\")\n",
    "    \n",
    "    # Definir archivos y sus nombres de colección en MongoDB\n",
    "    files_config = {\n",
    "        'customers': 'olist_customers_dataset.csv',\n",
    "        'geolocation': 'olist_geolocation_dataset.csv',\n",
    "        'order_items': 'olist_order_items_dataset.csv',\n",
    "        'order_payments': 'olist_order_payments_dataset.csv',\n",
    "        'order_reviews': 'olist_order_reviews_dataset.csv',\n",
    "        'orders': 'olist_orders_dataset.csv',\n",
    "        'products': 'olist_products_dataset.csv',\n",
    "        'sellers': 'olist_sellers_dataset.csv',\n",
    "        'product_categories': 'product_category_name_translation.csv'\n",
    "    }\n",
    "    \n",
    "    datasets = {}\n",
    "    total_rows = 0\n",
    "    \n",
    "    for dataset_name, filename in files_config.items():\n",
    "        try:\n",
    "            file_path = f'data/{filename}'\n",
    "            df = pd.read_csv(file_path)\n",
    "            datasets[dataset_name] = df\n",
    "            total_rows += len(df)\n",
    "            logger.info(f\"✅ {dataset_name}: {len(df):,} filas cargadas\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"❌ Archivo no encontrado: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error cargando {filename}: {str(e)}\")\n",
    "    \n",
    "    logger.info(f\"📊 Extracción completada: {len(datasets)} datasets, {total_rows:,} filas totales\")\n",
    "    return datasets\n",
    "\n",
    "# Ejecutar extracción\n",
    "raw_data = extract_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ea951",
   "metadata": {},
   "source": [
    "### 📥 **Función de Extracción de Datos**\n",
    "\n",
    "**`extract_data()` - Carga automática de datasets CSV**\n",
    "\n",
    "Esta función implementa la fase **Extract** del proceso ETL:\n",
    "\n",
    "**🔧 Funcionalidades:**\n",
    "- **Carga automática**: Lee todos los archivos CSV del dataset Olist\n",
    "- **Mapeo de archivos**: Asocia cada CSV con su nombre lógico en el sistema\n",
    "- **Manejo de errores**: Continúa el proceso aunque falten algunos archivos\n",
    "- **Logging detallado**: Registra el progreso y estadísticas de cada archivo\n",
    "- **Validación**: Verifica que los archivos existan antes de cargarlos\n",
    "\n",
    "**📊 Archivos procesados:**\n",
    "- `customers` - Datos de clientes (ubicación, identificación)\n",
    "- `orders` - Información de órdenes y estados\n",
    "- `products` - Catálogo de productos y categorías\n",
    "- `order_items` - Items individuales de cada orden\n",
    "- `payments` - Información de pagos y métodos\n",
    "- `reviews` - Reseñas y calificaciones de clientes\n",
    "- `sellers` - Datos de vendedores\n",
    "- `geolocation` - Información geográfica detallada\n",
    "- `product_categories` - Traducción de categorías al inglés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb27af",
   "metadata": {},
   "source": [
    "## 2. 🔧 TRANSFORMACIÓN (Transform)\n",
    "\n",
    "Limpieza, validación y transformación de datos para optimizar el almacenamiento en MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e242adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dates(df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte columnas de fecha a formato datetime y maneja valores nulos\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df_transformed.columns:\n",
    "            df_transformed[col] = pd.to_datetime(df_transformed[col], errors='coerce')\n",
    "            \n",
    "    return df_transformed\n",
    "\n",
    "def transform_customers(customers_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de clientes\n",
    "    \"\"\"\n",
    "    logger.info(\"🔄 Transformando datos de clientes...\")\n",
    "    \n",
    "    df = customers_df.copy()\n",
    "    \n",
    "    # Limpiar datos de ubicación\n",
    "    df['customer_city'] = df['customer_city'].str.lower().str.strip()\n",
    "    df['customer_state'] = df['customer_state'].str.upper().str.strip()\n",
    "    \n",
    "    # Agregar campos derivados\n",
    "    df['customer_location'] = df['customer_city'] + ', ' + df['customer_state']\n",
    "    \n",
    "    # Validar códigos postales (CEP brasileño)\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].astype(str).str.zfill(5)\n",
    "    \n",
    "    logger.info(f\"✅ Clientes transformados: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "def transform_orders(orders_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de órdenes\n",
    "    \"\"\"\n",
    "    logger.info(\"🔄 Transformando datos de órdenes...\")\n",
    "    \n",
    "    df = orders_df.copy()\n",
    "    \n",
    "    # Convertir fechas\n",
    "    date_columns = ['order_purchase_timestamp', 'order_approved_at', \n",
    "                   'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "                   'order_estimated_delivery_date']\n",
    "    \n",
    "    df = transform_dates(df, date_columns)\n",
    "    \n",
    "    # Agregar campos derivados\n",
    "    df['order_year'] = df['order_purchase_timestamp'].dt.year\n",
    "    df['order_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['order_weekday'] = df['order_purchase_timestamp'].dt.dayofweek\n",
    "    df['order_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    \n",
    "    # Calcular tiempos de entrega\n",
    "    df['days_to_deliver'] = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.days\n",
    "    df['days_vs_estimated'] = (df['order_delivered_customer_date'] - df['order_estimated_delivery_date']).dt.days\n",
    "    \n",
    "    # Categorizar estado de entrega\n",
    "    df['delivery_status'] = df.apply(lambda row: \n",
    "        'on_time' if pd.notna(row['days_vs_estimated']) and row['days_vs_estimated'] <= 0\n",
    "        else 'late' if pd.notna(row['days_vs_estimated']) and row['days_vs_estimated'] > 0\n",
    "        else 'unknown', axis=1)\n",
    "    \n",
    "    logger.info(f\"✅ Órdenes transformadas: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "def transform_products(products_df: pd.DataFrame, categories_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de productos y añade categorías en inglés\n",
    "    \"\"\"\n",
    "    logger.info(\"🔄 Transformando datos de productos...\")\n",
    "    \n",
    "    df = products_df.copy()\n",
    "    \n",
    "    # Unir con traducciones de categorías\n",
    "    df = df.merge(categories_df, on='product_category_name', how='left')\n",
    "    \n",
    "    # Limpiar y estandarizar categorías\n",
    "    df['product_category_name'] = df['product_category_name'].fillna('unknown')\n",
    "    df['product_category_name_english'] = df['product_category_name_english'].fillna('unknown')\n",
    "    \n",
    "    # Calcular volumen del producto\n",
    "    df['product_volume_cm3'] = (df['product_length_cm'] * \n",
    "                               df['product_height_cm'] * \n",
    "                               df['product_width_cm'])\n",
    "    \n",
    "    # Categorizar tamaño por peso\n",
    "    df['weight_category'] = pd.cut(df['product_weight_g'], \n",
    "                                  bins=[0, 500, 2000, 10000, float('inf')],\n",
    "                                  labels=['light', 'medium', 'heavy', 'very_heavy'])\n",
    "    \n",
    "    logger.info(f\"✅ Productos transformados: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "def transform_order_items(order_items_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de items de órdenes\n",
    "    \"\"\"\n",
    "    logger.info(\"🔄 Transformando datos de items...\")\n",
    "    \n",
    "    df = order_items_df.copy()\n",
    "    \n",
    "    # Calcular métricas de precio\n",
    "    df['unit_price'] = df['price'] / df['order_item_id']  # Precio por unidad\n",
    "    df['total_item_value'] = df['price'] + df['freight_value']  # Valor total con envío\n",
    "    \n",
    "    # Categorizar valor del envío\n",
    "    df['freight_category'] = pd.cut(df['freight_value'], \n",
    "                                   bins=[0, 10, 30, 100, float('inf')],\n",
    "                                   labels=['low', 'medium', 'high', 'very_high'])\n",
    "    \n",
    "    logger.info(f\"✅ Items transformados: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "def transform_payments(payments_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de pagos\n",
    "    \"\"\"\n",
    "    logger.info(\"🔄 Transformando datos de pagos...\")\n",
    "    \n",
    "    df = payments_df.copy()\n",
    "    \n",
    "    # Categorizar valores de pago\n",
    "    df['payment_range'] = pd.cut(df['payment_value'], \n",
    "                                bins=[0, 50, 100, 200, 500, float('inf')],\n",
    "                                labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "    \n",
    "    # Normalizar tipos de pago\n",
    "    df['payment_type'] = df['payment_type'].str.lower().str.replace('_', ' ')\n",
    "    \n",
    "    logger.info(f\"✅ Pagos transformados: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "def transform_reviews(reviews_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de reseñas\n",
    "    \"\"\"\n",
    "    logger.info(\"🔄 Transformando datos de reseñas...\")\n",
    "    \n",
    "    df = reviews_df.copy()\n",
    "    \n",
    "    # Convertir fechas\n",
    "    date_columns = ['review_creation_date', 'review_answer_timestamp']\n",
    "    df = transform_dates(df, date_columns)\n",
    "    \n",
    "    # Categorizar satisfacción\n",
    "    df['satisfaction_level'] = df['review_score'].map({\n",
    "        1: 'very_dissatisfied',\n",
    "        2: 'dissatisfied', \n",
    "        3: 'neutral',\n",
    "        4: 'satisfied',\n",
    "        5: 'very_satisfied'\n",
    "    })\n",
    "    \n",
    "    # Analizar longitud de comentarios\n",
    "    df['comment_title_length'] = df['review_comment_title'].str.len().fillna(0)\n",
    "    df['comment_message_length'] = df['review_comment_message'].str.len().fillna(0)\n",
    "    df['has_comment'] = (df['comment_title_length'] + df['comment_message_length']) > 0\n",
    "    \n",
    "    logger.info(f\"✅ Reseñas transformadas: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "# Aplicar todas las transformaciones\n",
    "def transform_all_data(raw_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aplica todas las transformaciones a los datasets\n",
    "    \"\"\"\n",
    "    logger.info(\"🔄 Iniciando transformación completa de datos...\")\n",
    "    \n",
    "    transformed_data = {}\n",
    "    \n",
    "    # Transformar cada dataset\n",
    "    transformed_data['customers'] = transform_customers(raw_data['customers'])\n",
    "    transformed_data['orders'] = transform_orders(raw_data['orders'])\n",
    "    transformed_data['products'] = transform_products(raw_data['products'], raw_data['product_categories'])\n",
    "    transformed_data['order_items'] = transform_order_items(raw_data['order_items'])\n",
    "    transformed_data['payments'] = transform_payments(raw_data['order_payments'])\n",
    "    transformed_data['reviews'] = transform_reviews(raw_data['order_reviews'])\n",
    "    \n",
    "    # Datasets que no requieren transformación especial\n",
    "    transformed_data['sellers'] = raw_data['sellers'].copy()\n",
    "    transformed_data['geolocation'] = raw_data['geolocation'].copy()\n",
    "    \n",
    "    logger.info(\"✅ Transformación completa finalizada\")\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419cc4c",
   "metadata": {},
   "source": [
    "### 🔧 **Funciones de Transformación de Datos**\n",
    "\n",
    "Esta sección contiene todas las funciones especializadas para la fase **Transform** del ETL:\n",
    "\n",
    "#### **🔄 `transform_dates()`** \n",
    "Convierte columnas de texto a formato datetime, manejando valores nulos y errores de formato.\n",
    "\n",
    "#### **👥 `transform_customers()`**\n",
    "- Estandariza nombres de ciudades (minúsculas) y estados (mayúsculas)\n",
    "- Crea campo combinado de ubicación\n",
    "- Valida y formatea códigos postales brasileños (CEP)\n",
    "- Agrega campos derivados para análisis geográfico\n",
    "\n",
    "#### **📦 `transform_orders()`**\n",
    "- Convierte timestamps de órdenes a formato datetime\n",
    "- Extrae componentes temporales (año, mes, día de semana, hora)\n",
    "- Calcula métricas de entrega (días de entrega, comparación con estimado)\n",
    "- Categoriza el estado de entrega (a tiempo, tardío, desconocido)\n",
    "\n",
    "#### **🛍️ `transform_products()`**\n",
    "- Une productos con traducciones de categorías al inglés\n",
    "- Calcula volumen del producto (cm³)\n",
    "- Categoriza productos por peso (ligero, medio, pesado, muy pesado)\n",
    "- Estandariza nombres de categorías\n",
    "\n",
    "#### **📋 `transform_order_items()`**\n",
    "- Calcula precio unitario y valor total con envío\n",
    "- Categoriza costos de envío por rangos\n",
    "- Agrega métricas de valor por item\n",
    "\n",
    "#### **💳 `transform_payments()`**\n",
    "- Categoriza valores de pago por rangos\n",
    "- Normaliza tipos de pago\n",
    "- Estandariza formato de métodos de pago\n",
    "\n",
    "#### **⭐ `transform_reviews()`**\n",
    "- Convierte fechas de reseñas\n",
    "- Mapea scores numéricos a niveles de satisfacción\n",
    "- Analiza longitud de comentarios\n",
    "- Detecta presencia de comentarios textuales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d593d3",
   "metadata": {},
   "source": [
    "## 3. 📤 CARGA (Load) - Configuración MongoDB\n",
    "\n",
    "Configuración de conexión y carga de datos a MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ce5fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Configurando conexión a MongoDB...\n",
      "📍 Host: localhost:27020\n",
      "🗄️ Base de datos: olist_ecommerce\n",
      "\n",
      "⚠️  NOTA: Asegúrate de que MongoDB esté ejecutándose antes de continuar\n"
     ]
    }
   ],
   "source": [
    "# Configuración de MongoDB\n",
    "MONGODB_CONFIG = {\n",
    "    'host': 'localhost',  # Cambiar por tu host de MongoDB\n",
    "    'port': 27020,        # Puerto por defecto de MongoDB\n",
    "    'database': 'olist_ecommerce',  # Nombre de la base de datos\n",
    "    'username': None,     # Agregar si usas autenticación\n",
    "    'password': None      # Agregar si usas autenticación\n",
    "}\n",
    "\n",
    "def get_mongodb_connection():\n",
    "    \"\"\"\n",
    "    Establece conexión con MongoDB\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construir URI de conexión\n",
    "        if MONGODB_CONFIG['username'] and MONGODB_CONFIG['password']:\n",
    "            connection_string = f\"mongodb://{MONGODB_CONFIG['username']}:{MONGODB_CONFIG['password']}@{MONGODB_CONFIG['host']}:{MONGODB_CONFIG['port']}/{MONGODB_CONFIG['database']}\"\n",
    "        else:\n",
    "            connection_string = f\"mongodb://{MONGODB_CONFIG['host']}:{MONGODB_CONFIG['port']}/\"\n",
    "        \n",
    "        # Conectar a MongoDB\n",
    "        client = MongoClient(connection_string)\n",
    "        \n",
    "        # Verificar conexión\n",
    "        client.admin.command('ping')\n",
    "        \n",
    "        # Obtener base de datos\n",
    "        db = client[MONGODB_CONFIG['database']]\n",
    "        \n",
    "        logger.info(f\"✅ Conexión exitosa a MongoDB: {MONGODB_CONFIG['database']}\")\n",
    "        return client, db\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error conectando a MongoDB: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def create_indexes(db):\n",
    "    \"\"\"\n",
    "    Crea índices para optimizar consultas\n",
    "    \"\"\"\n",
    "    logger.info(\"🔄 Creando índices para optimizar consultas...\")\n",
    "    \n",
    "    try:\n",
    "        # Índices para órdenes\n",
    "        db.orders.create_index(\"customer_id\")\n",
    "        db.orders.create_index(\"order_status\")\n",
    "        db.orders.create_index(\"order_purchase_timestamp\")\n",
    "        db.orders.create_index([(\"order_year\", 1), (\"order_month\", 1)])\n",
    "        \n",
    "        # Índices para items\n",
    "        db.order_items.create_index(\"order_id\")\n",
    "        db.order_items.create_index(\"product_id\")\n",
    "        db.order_items.create_index(\"seller_id\")\n",
    "        \n",
    "        # Índices para pagos\n",
    "        db.payments.create_index(\"order_id\")\n",
    "        db.payments.create_index(\"payment_type\")\n",
    "        \n",
    "        # Índices para reseñas\n",
    "        db.reviews.create_index(\"order_id\")\n",
    "        db.reviews.create_index(\"review_score\")\n",
    "        \n",
    "        # Índices para clientes\n",
    "        db.customers.create_index(\"customer_state\")\n",
    "        db.customers.create_index(\"customer_city\")\n",
    "        \n",
    "        # Índices para productos\n",
    "        db.products.create_index(\"product_category_name_english\")\n",
    "        \n",
    "        logger.info(\"✅ Índices creados exitosamente\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error creando índices: {str(e)}\")\n",
    "\n",
    "# Establecer conexión inicial\n",
    "print(\"🔄 Configurando conexión a MongoDB...\")\n",
    "print(f\"📍 Host: {MONGODB_CONFIG['host']}:{MONGODB_CONFIG['port']}\")\n",
    "print(f\"🗄️ Base de datos: {MONGODB_CONFIG['database']}\")\n",
    "print(\"\\n⚠️  NOTA: Asegúrate de que MongoDB esté ejecutándose antes de continuar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991de62",
   "metadata": {},
   "source": [
    "### 🗄️ **Configuración y Conexión a MongoDB**\n",
    "\n",
    "Esta sección establece la configuración de conexión a MongoDB y las funciones de conectividad:\n",
    "\n",
    "#### **⚙️ `MONGODB_CONFIG`**\n",
    "Diccionario de configuración centralizada que contiene:\n",
    "- **host**: Dirección del servidor MongoDB (localhost para desarrollo)\n",
    "- **port**: Puerto de conexión (27020 configurado para el cluster de réplicas)\n",
    "- **database**: Nombre de la base de datos objetivo (`olist_ecommerce`)\n",
    "- **username/password**: Credenciales de autenticación (None para conexión sin auth)\n",
    "\n",
    "#### **🔌 `get_mongodb_connection()`**\n",
    "Función robusta de conexión que:\n",
    "- Construye la URI de conexión según las credenciales disponibles\n",
    "- Establece la conexión y verifica conectividad con `ping`\n",
    "- Maneja errores de conexión con logging detallado\n",
    "- Retorna cliente y base de datos para uso posterior\n",
    "\n",
    "#### **📊 `create_indexes()`**\n",
    "Optimización de rendimiento mediante índices:\n",
    "- **Órdenes**: customer_id, order_status, timestamps, campos temporales\n",
    "- **Items**: order_id, product_id, seller_id para joins eficientes\n",
    "- **Pagos**: order_id, payment_type para agregaciones rápidas\n",
    "- **Reseñas**: order_id, review_score para análisis de satisfacción\n",
    "- **Clientes**: customer_state, customer_city para análisis geográfico\n",
    "- **Productos**: product_category_name_english para filtros de categoría\n",
    "\n",
    "**💡 Nota importante**: Los índices se crean después de la carga de datos para mejor rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a0a8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_for_mongodb(df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Prepara un DataFrame para inserción en MongoDB\n",
    "    \"\"\"\n",
    "    # Convertir DataFrame a diccionarios\n",
    "    records = df.to_dict('records')\n",
    "    \n",
    "    # Convertir tipos de datos problemáticos\n",
    "    for record in records:\n",
    "        for key, value in record.items():\n",
    "            # Convertir NaN y tipos numpy a tipos nativos de Python\n",
    "            if pd.isna(value):\n",
    "                record[key] = None\n",
    "            elif isinstance(value, (np.integer, np.floating)):\n",
    "                record[key] = value.item()\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                record[key] = value.tolist()\n",
    "            elif hasattr(value, 'isoformat'):  # datetime objects\n",
    "                record[key] = value\n",
    "    \n",
    "    return records\n",
    "\n",
    "def load_to_mongodb(db, collection_name: str, data: pd.DataFrame, batch_size: int = 1000) -> bool:\n",
    "    \"\"\"\n",
    "    Carga datos a una colección de MongoDB en lotes con retroalimentación mejorada\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_records = len(data)\n",
    "        logger.info(f\"🔄 Iniciando carga de {total_records:,} registros a '{collection_name}'...\")\n",
    "        \n",
    "        # Preparar datos para MongoDB\n",
    "        print(f\"   📋 Preparando datos para MongoDB...\")\n",
    "        records = prepare_dataframe_for_mongodb(data)\n",
    "        print(f\"   ✅ Datos preparados: {len(records):,} documentos listos\")\n",
    "        \n",
    "        # Obtener colección\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        # Limpiar colección existente (opcional)\n",
    "        print(f\"   🗑️ Limpiando colección existente '{collection_name}'...\")\n",
    "        collection.drop()\n",
    "        logger.info(f\"   ✅ Colección '{collection_name}' limpiada\")\n",
    "        \n",
    "        # Calcular número de lotes\n",
    "        total_batches = (len(records) + batch_size - 1) // batch_size\n",
    "        print(f\"   📦 Dividiendo en {total_batches} lotes de {batch_size:,} registros\")\n",
    "        \n",
    "        # Insertar en lotes con barra de progreso\n",
    "        total_inserted = 0\n",
    "        failed_batches = 0\n",
    "        \n",
    "        print(f\"   🚀 Iniciando inserción en lotes...\")\n",
    "        print(f\"   {'='*50}\")\n",
    "        \n",
    "        for batch_num in range(total_batches):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(records))\n",
    "            batch = records[start_idx:end_idx]\n",
    "            \n",
    "            try:\n",
    "                # Insertar lote\n",
    "                result = collection.insert_many(batch, ordered=False)\n",
    "                batch_inserted = len(result.inserted_ids)\n",
    "                total_inserted += batch_inserted\n",
    "                \n",
    "                # Calcular progreso\n",
    "                progress_percent = ((batch_num + 1) / total_batches) * 100\n",
    "                \n",
    "                # Mostrar progreso cada 10% o cada 10 lotes\n",
    "                if (batch_num + 1) % max(1, total_batches // 10) == 0 or batch_num == total_batches - 1:\n",
    "                    progress_bar = \"█\" * int(progress_percent // 5) + \"░\" * (20 - int(progress_percent // 5))\n",
    "                    print(f\"   📊 [{progress_bar}] {progress_percent:.1f}% - \"\n",
    "                          f\"Lote {batch_num + 1}/{total_batches} - \"\n",
    "                          f\"{total_inserted:,}/{total_records:,} registros\")\n",
    "                \n",
    "            except Exception as batch_error:\n",
    "                failed_batches += 1\n",
    "                logger.warning(f\"   ⚠️ Error en lote {batch_num + 1}: {str(batch_error)}\")\n",
    "                \n",
    "                # Intentar insertar documentos uno por uno en caso de error\n",
    "                individual_inserted = 0\n",
    "                for doc in batch:\n",
    "                    try:\n",
    "                        collection.insert_one(doc)\n",
    "                        individual_inserted += 1\n",
    "                        total_inserted += 1\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                if individual_inserted > 0:\n",
    "                    logger.info(f\"   🔧 Recuperados {individual_inserted} docs del lote {batch_num + 1}\")\n",
    "        \n",
    "        print(f\"   {'='*50}\")\n",
    "        \n",
    "        # Verificar inserción final\n",
    "        final_count = collection.count_documents({})\n",
    "        \n",
    "        if failed_batches > 0:\n",
    "            logger.warning(f\"   ⚠️ {failed_batches} lotes tuvieron errores parciales\")\n",
    "        \n",
    "        # Resumen final\n",
    "        print(f\"   ✅ Carga completada para '{collection_name}':\")\n",
    "        print(f\"      📊 Documentos insertados: {total_inserted:,}\")\n",
    "        print(f\"      🔍 Documentos verificados: {final_count:,}\")\n",
    "        print(f\"      📦 Lotes procesados: {total_batches}\")\n",
    "        print(f\"      ❌ Lotes con errores: {failed_batches}\")\n",
    "        \n",
    "        if final_count != total_inserted:\n",
    "            logger.warning(f\"   ⚠️ Discrepancia: esperados {total_inserted:,}, encontrados {final_count:,}\")\n",
    "        \n",
    "        logger.info(f\"✅ Carga completada: {final_count:,} registros en '{collection_name}'\")\n",
    "        return final_count > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error crítico cargando datos a '{collection_name}': {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39aaa1b",
   "metadata": {},
   "source": [
    "### 📤 **Funciones de Carga a MongoDB (Load)**\n",
    "\n",
    "Esta sección implementa la fase **Load** del ETL con carga optimizada por lotes:\n",
    "\n",
    "#### **🔄 `prepare_dataframe_for_mongodb()`**\n",
    "Prepara los datos de pandas para inserción en MongoDB:\n",
    "- **Conversión de tipos**: Transforma DataFrames a diccionarios JSON-compatibles\n",
    "- **Manejo de NaN**: Convierte valores NaN a None (null en MongoDB)\n",
    "- **Tipos numpy**: Convierte tipos numpy a tipos nativos de Python\n",
    "- **Fechas**: Preserva objetos datetime para MongoDB\n",
    "- **Arrays**: Convierte arrays numpy a listas Python\n",
    "\n",
    "#### **📦 `load_to_mongodb()` - Carga Optimizada por Lotes**\n",
    "Función avanzada de carga con las siguientes características:\n",
    "\n",
    "**🚀 Optimizaciones de rendimiento:**\n",
    "- **Carga por lotes**: Divide grandes datasets en lotes manejables\n",
    "- **Inserción masiva**: Usa `insert_many()` para máxima eficiencia\n",
    "- **Ordered=False**: Permite inserción paralela para mejor velocidad\n",
    "- **Tamaños adaptativos**: Lotes optimizados según el tipo de datos\n",
    "\n",
    "**📊 Retroalimentación en tiempo real:**\n",
    "- **Barra de progreso visual**: Muestra porcentaje completado\n",
    "- **Estadísticas por lote**: Documentos insertados, velocidad, tiempo\n",
    "- **Métricas de rendimiento**: Documentos por segundo\n",
    "- **Progreso detallado**: Información de cada lote procesado\n",
    "\n",
    "**🛡️ Manejo robusto de errores:**\n",
    "- **Recuperación automática**: Si un lote falla, intenta inserción individual\n",
    "- **Continuación del proceso**: Los errores no detienen la carga completa\n",
    "- **Logging detallado**: Registra errores y recuperaciones exitosas\n",
    "- **Verificación final**: Cuenta documentos insertados vs. esperados\n",
    "\n",
    "**📋 Proceso de carga:**\n",
    "1. Preparación de datos para MongoDB\n",
    "2. Limpieza de colección existente\n",
    "3. División en lotes optimizados\n",
    "4. Inserción con monitoreo de progreso\n",
    "5. Manejo de errores y recuperación\n",
    "6. Verificación y reporte final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "551aba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data_to_mongodb_enhanced(transformed_data: Dict[str, pd.DataFrame]) -> bool:\n",
    "    \"\"\"\n",
    "    Carga todos los datasets transformados a MongoDB con retroalimentación mejorada\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(\"🚀 Iniciando carga completa a MongoDB con retroalimentación mejorada...\")\n",
    "    \n",
    "    # Establecer conexión\n",
    "    client, db = get_mongodb_connection()\n",
    "    \n",
    "    if db is None:\n",
    "        logger.error(\"❌ No se pudo establecer conexión con MongoDB\")\n",
    "        return False\n",
    "    \n",
    "    # Mapeo de datasets a nombres de colecciones con tamaños de lote optimizados\n",
    "    collection_config = {\n",
    "        'customers': {'collection': 'customers', 'batch_size': 500},\n",
    "        'orders': {'collection': 'orders', 'batch_size': 300},\n",
    "        'products': {'collection': 'products', 'batch_size': 500},\n",
    "        'order_items': {'collection': 'order_items', 'batch_size': 200},\n",
    "        'payments': {'collection': 'payments', 'batch_size': 300},\n",
    "        'reviews': {'collection': 'reviews', 'batch_size': 300},\n",
    "        'sellers': {'collection': 'sellers', 'batch_size': 1000},\n",
    "        'geolocation': {'collection': 'geolocation', 'batch_size': 100}  # Dataset más grande, lotes pequeños\n",
    "    }\n",
    "    \n",
    "    success_count = 0\n",
    "    total_collections = len([k for k in transformed_data.keys() if k in collection_config])\n",
    "    total_documents_loaded = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🚀 PROCESO DE CARGA MASIVA A MONGODB\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"📊 Total de colecciones a cargar: {total_collections}\")\n",
    "    print(f\"⏱️ Iniciado: {start_time.strftime('%H:%M:%S')}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Cargar cada dataset\n",
    "        for dataset_num, (dataset_name, df) in enumerate(transformed_data.items(), 1):\n",
    "            if dataset_name in collection_config:\n",
    "                config = collection_config[dataset_name]\n",
    "                collection_name = config['collection']\n",
    "                batch_size = config['batch_size']\n",
    "                \n",
    "                print(f\"\\n📋 COLECCIÓN {dataset_num}/{total_collections}: {collection_name.upper()}\")\n",
    "                print(f\"   📊 Registros a cargar: {len(df):,}\")\n",
    "                print(f\"   📦 Tamaño de lote: {batch_size:,}\")\n",
    "                \n",
    "                collection_start = datetime.now()\n",
    "                \n",
    "                if load_to_mongodb(db, collection_name, df, batch_size):\n",
    "                    success_count += 1\n",
    "                    total_documents_loaded += len(df)\n",
    "                    \n",
    "                    collection_duration = datetime.now() - collection_start\n",
    "                    rate = len(df) / collection_duration.total_seconds() if collection_duration.total_seconds() > 0 else 0\n",
    "                    \n",
    "                    print(f\"   ✅ Completada en {collection_duration.total_seconds():.1f}s \"\n",
    "                          f\"({rate:.0f} docs/sec)\")\n",
    "                else:\n",
    "                    print(f\"   ❌ FALLÓ la carga de {dataset_name}\")\n",
    "                    logger.error(f\"❌ Falló la carga de {dataset_name}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "        # Crear índices después de cargar todos los datos\n",
    "        if success_count == total_collections:\n",
    "            print(\"🔧 CREANDO ÍNDICES PARA OPTIMIZAR CONSULTAS...\")\n",
    "            index_start = datetime.now()\n",
    "            create_indexes(db)\n",
    "            index_duration = datetime.now() - index_start\n",
    "            print(f\"✅ Índices creados en {index_duration.total_seconds():.1f}s\")\n",
    "            \n",
    "        # Generar estadísticas finales detalladas\n",
    "        print(\"\\n📊 ESTADÍSTICAS FINALES DE CARGA:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        grand_total = 0\n",
    "        for dataset_name, config in collection_config.items():\n",
    "            if dataset_name in transformed_data:\n",
    "                collection_name = config['collection']\n",
    "                try:\n",
    "                    count = db[collection_name].count_documents({})\n",
    "                    grand_total += count\n",
    "                    status = \"✅\" if count > 0 else \"❌\"\n",
    "                    print(f\"{status} {collection_name:15}: {count:,} documentos\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ {collection_name:15}: Error verificando - {str(e)}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(f\"📊 TOTAL DOCUMENTOS: {grand_total:,}\")\n",
    "        \n",
    "        # Resumen final del proceso\n",
    "        total_duration = datetime.now() - start_time\n",
    "        overall_rate = total_documents_loaded / total_duration.total_seconds() if total_duration.total_seconds() > 0 else 0\n",
    "        \n",
    "        print(f\"\\n🎯 RESUMEN DEL PROCESO:\")\n",
    "        print(f\"   ✅ Colecciones exitosas: {success_count}/{total_collections}\")\n",
    "        print(f\"   📊 Documentos cargados: {total_documents_loaded:,}\")\n",
    "        print(f\"   ⏱️ Tiempo total: {total_duration}\")\n",
    "        print(f\"   🚀 Velocidad promedio: {overall_rate:.0f} docs/segundo\")\n",
    "        \n",
    "        if success_count == total_collections:\n",
    "            print(f\"\\n🎉 ¡PROCESO COMPLETADO EXITOSAMENTE!\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️ PROCESO PARCIALMENTE COMPLETADO\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return success_count == total_collections\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error crítico en proceso de carga: {str(e)}\")\n",
    "        print(f\"❌ Error crítico: {str(e)}\")\n",
    "        return False\n",
    "        \n",
    "    finally:\n",
    "        # Cerrar conexión\n",
    "        if client:\n",
    "            client.close()\n",
    "            logger.info(\"🔌 Conexión a MongoDB cerrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a579684",
   "metadata": {},
   "source": [
    "### 🚀 **Carga Completa con Retroalimentación Avanzada**\n",
    "\n",
    "#### **📋 `load_all_data_to_mongodb_enhanced()` - Orquestador Principal**\n",
    "\n",
    "Esta función coordina la carga completa de todos los datasets con características avanzadas:\n",
    "\n",
    "**⚙️ Configuración adaptativa por colección:**\n",
    "```python\n",
    "collection_config = {\n",
    "    'customers': {'collection': 'customers', 'batch_size': 500},     # Datos simples\n",
    "    'orders': {'collection': 'orders', 'batch_size': 300},          # Muchas fechas\n",
    "    'products': {'collection': 'products', 'batch_size': 500},      # Tamaño medio\n",
    "    'order_items': {'collection': 'order_items', 'batch_size': 200}, # Dataset grande\n",
    "    'payments': {'collection': 'payments', 'batch_size': 300},      # Datos financieros\n",
    "    'reviews': {'collection': 'reviews', 'batch_size': 300},        # Texto variable\n",
    "    'sellers': {'collection': 'sellers', 'batch_size': 1000},       # Datos simples\n",
    "    'geolocation': {'collection': 'geolocation', 'batch_size': 100} # Dataset muy grande\n",
    "}\n",
    "```\n",
    "\n",
    "**📊 Características principales:**\n",
    "\n",
    "1. **Dashboard de progreso en tiempo real:**\n",
    "   - Contador de colecciones (1/8, 2/8, etc.)\n",
    "   - Progreso individual por colección\n",
    "   - Métricas de velocidad (documentos/segundo)\n",
    "   - Tiempo de procesamiento por colección\n",
    "\n",
    "2. **Gestión inteligente de recursos:**\n",
    "   - Tamaños de lote optimizados por tipo de datos\n",
    "   - Conexión única reutilizada para todas las colecciones\n",
    "   - Limpieza automática de memoria entre procesos\n",
    "\n",
    "3. **Monitoreo y estadísticas:**\n",
    "   - Tiempo de inicio y duración total\n",
    "   - Velocidad promedio del proceso completo\n",
    "   - Conteo final de documentos por colección\n",
    "   - Verificación de integridad post-carga\n",
    "\n",
    "4. **Post-procesamiento automático:**\n",
    "   - Creación de índices después de la carga\n",
    "   - Validación de conteos de documentos\n",
    "   - Generación de reporte final detallado\n",
    "   - Cierre seguro de conexiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3bfdf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ FUNCIONES ETL MEJORADAS DISPONIBLES:\n",
      "==================================================\n",
      "📋 Funciones principales:\n",
      "   • run_complete_etl_pipeline() - ETL completo con retroalimentación mejorada\n",
      "   • run_quick_etl_test() - ETL rápido para pruebas\n",
      "   • load_all_data_to_mongodb_enhanced() - Solo carga con mejor feedback\n",
      "\n",
      "🔧 Mejoras implementadas:\n",
      "   ✅ Carga por lotes optimizada\n",
      "   ✅ Barra de progreso visual\n",
      "   ✅ Estadísticas en tiempo real\n",
      "   ✅ Manejo de errores por lote\n",
      "   ✅ Recuperación automática de errores\n",
      "   ✅ Métricas de velocidad de carga\n",
      "   ✅ Resumen detallado final\n",
      "\n",
      "📝 Para ejecutar:\n",
      "   # ETL completo mejorado:\n",
      "   etl_success = run_complete_etl_pipeline()\n",
      "\n",
      "   # Prueba rápida:\n",
      "   test_success = run_quick_etl_test(['customers', 'orders'])\n"
     ]
    }
   ],
   "source": [
    "def run_complete_etl_pipeline():\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline ETL completo con retroalimentación mejorada\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(\"🚀 INICIANDO PROCESO ETL COMPLETO CON RETROALIMENTACIÓN MEJORADA\")\n",
    "    logger.info(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # PASO 1: EXTRACCIÓN\n",
    "        print(\"\\n📥 FASE 1: EXTRACCIÓN DE DATOS\")\n",
    "        print(\"=\"*40)\n",
    "        extraction_start = datetime.now()\n",
    "        \n",
    "        raw_data = extract_data()\n",
    "        \n",
    "        if not raw_data:\n",
    "            logger.error(\"❌ Error en extracción de datos\")\n",
    "            return False\n",
    "        \n",
    "        extraction_duration = datetime.now() - extraction_start\n",
    "        print(f\"✅ Extracción completada en {extraction_duration.total_seconds():.1f}s\")\n",
    "        \n",
    "        # PASO 2: TRANSFORMACIÓN\n",
    "        print(\"\\n🔧 FASE 2: TRANSFORMACIÓN DE DATOS\")\n",
    "        print(\"=\"*40)\n",
    "        transformation_start = datetime.now()\n",
    "        \n",
    "        transformed_data = transform_all_data(raw_data)\n",
    "        \n",
    "        transformation_duration = datetime.now() - transformation_start\n",
    "        print(f\"✅ Transformación completada en {transformation_duration.total_seconds():.1f}s\")\n",
    "        \n",
    "        # PASO 3: CARGA CON RETROALIMENTACIÓN MEJORADA\n",
    "        print(\"\\n📤 FASE 3: CARGA A MONGODB CON RETROALIMENTACIÓN MEJORADA\")\n",
    "        print(\"=\"*60)\n",
    "        loading_start = datetime.now()\n",
    "        \n",
    "        # Usar la función de carga mejorada\n",
    "        success = load_all_data_to_mongodb_enhanced(transformed_data)\n",
    "        \n",
    "        loading_duration = datetime.now() - loading_start\n",
    "        print(f\"\\n⏱️ Carga completada en {loading_duration.total_seconds():.1f}s\")\n",
    "        \n",
    "        # RESUMEN FINAL DETALLADO\n",
    "        end_time = datetime.now()\n",
    "        total_duration = end_time - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"📊 RESUMEN COMPLETO DEL PROCESO ETL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"⏱️ TIEMPOS DE EJECUCIÓN:\")\n",
    "        print(f\"   📥 Extracción:     {extraction_duration.total_seconds():6.1f}s\")\n",
    "        print(f\"   🔧 Transformación: {transformation_duration.total_seconds():6.1f}s\") \n",
    "        print(f\"   📤 Carga:          {loading_duration.total_seconds():6.1f}s\")\n",
    "        print(f\"   ⏱️ TOTAL:          {total_duration.total_seconds():6.1f}s\")\n",
    "        \n",
    "        print(f\"\\n📅 TIMESTAMPS:\")\n",
    "        print(f\"   🚀 Iniciado:   {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"   🏁 Finalizado: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\n🎉 ¡PROCESO ETL COMPLETADO EXITOSAMENTE!\")\n",
    "            print(f\"✅ Todos los datos están disponibles en MongoDB\")\n",
    "            print(f\"🗄️ Base de datos: '{MONGODB_CONFIG['database']}'\")\n",
    "            print(f\"🔗 Host: {MONGODB_CONFIG['host']}:{MONGODB_CONFIG['port']}\")\n",
    "        else:\n",
    "            print(f\"\\n❌ PROCESO ETL FALLÓ\")\n",
    "            print(f\"⚠️ Revisa los logs para más detalles\")\n",
    "            \n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error crítico en pipeline ETL: {str(e)}\")\n",
    "        print(f\"\\n❌ ERROR CRÍTICO: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# FUNCIÓN DE EJECUCIÓN RÁPIDA PARA PRUEBAS\n",
    "def run_quick_etl_test(collections_to_test: List[str] = None):\n",
    "    \"\"\"\n",
    "    Ejecuta una versión rápida del ETL para pruebas con colecciones específicas\n",
    "    \"\"\"\n",
    "    if collections_to_test is None:\n",
    "        collections_to_test = ['customers', 'orders']  # Solo unas pocas para prueba rápida\n",
    "    \n",
    "    print(\"🧪 EJECUTANDO ETL DE PRUEBA RÁPIDA\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"📋 Colecciones a probar: {', '.join(collections_to_test)}\")\n",
    "    \n",
    "    try:\n",
    "        # Extracción\n",
    "        raw_data = extract_data()\n",
    "        if not raw_data:\n",
    "            return False\n",
    "        \n",
    "        # Transformación\n",
    "        transformed_data = transform_all_data(raw_data)\n",
    "        \n",
    "        # Filtrar solo las colecciones de prueba\n",
    "        test_data = {k: v for k, v in transformed_data.items() if k in collections_to_test}\n",
    "        \n",
    "        # Carga con lotes pequeños para prueba rápida\n",
    "        success = load_all_data_to_mongodb_enhanced(test_data)\n",
    "        \n",
    "        if success:\n",
    "            print(\"✅ Prueba ETL completada exitosamente\")\n",
    "        else:\n",
    "            print(\"❌ Prueba ETL falló\")\n",
    "            \n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en prueba ETL: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# INFORMACIÓN PARA EL USUARIO\n",
    "print(\"🛠️ FUNCIONES ETL MEJORADAS DISPONIBLES:\")\n",
    "print(\"=\"*50)\n",
    "print(\"📋 Funciones principales:\")\n",
    "print(\"   • run_complete_etl_pipeline() - ETL completo con retroalimentación mejorada\")\n",
    "print(\"   • run_quick_etl_test() - ETL rápido para pruebas\")\n",
    "print(\"   • load_all_data_to_mongodb_enhanced() - Solo carga con mejor feedback\")\n",
    "print()\n",
    "print(\"🔧 Mejoras implementadas:\")\n",
    "print(\"   ✅ Carga por lotes optimizada\")\n",
    "print(\"   ✅ Barra de progreso visual\")\n",
    "print(\"   ✅ Estadísticas en tiempo real\")\n",
    "print(\"   ✅ Manejo de errores por lote\")\n",
    "print(\"   ✅ Recuperación automática de errores\")\n",
    "print(\"   ✅ Métricas de velocidad de carga\")\n",
    "print(\"   ✅ Resumen detallado final\")\n",
    "print()\n",
    "print(\"📝 Para ejecutar:\")\n",
    "print(\"   # ETL completo mejorado:\")\n",
    "print(\"   etl_success = run_complete_etl_pipeline()\")\n",
    "print()\n",
    "print(\"   # Prueba rápida:\")\n",
    "print(\"   test_success = run_quick_etl_test(['customers', 'orders'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c5e6d",
   "metadata": {},
   "source": [
    "### 🎯 **Pipeline ETL Principal y Funciones de Control**\n",
    "\n",
    "Esta sección contiene las funciones orquestadoras que coordinan todo el proceso ETL:\n",
    "\n",
    "#### **🚀 `run_complete_etl_pipeline()` - Función Principal**\n",
    "\n",
    "Ejecuta el proceso ETL completo de principio a fin con monitoreo avanzado:\n",
    "\n",
    "**📋 Fases del proceso:**\n",
    "\n",
    "1. **📥 FASE 1: EXTRACCIÓN**\n",
    "   - Carga todos los archivos CSV del dataset Olist\n",
    "   - Valida existencia y formato de archivos\n",
    "   - Registra estadísticas de carga por archivo\n",
    "   - Mide tiempo de extracción\n",
    "\n",
    "2. **🔧 FASE 2: TRANSFORMACIÓN**\n",
    "   - Aplica todas las transformaciones específicas por dataset\n",
    "   - Limpia y estandariza datos\n",
    "   - Crea campos derivados y métricas de negocio\n",
    "   - Mide tiempo de transformación\n",
    "\n",
    "3. **📤 FASE 3: CARGA MEJORADA**\n",
    "   - Utiliza `load_all_data_to_mongodb_enhanced()`\n",
    "   - Carga optimizada por lotes con retroalimentación\n",
    "   - Creación automática de índices\n",
    "   - Mide tiempo de carga\n",
    "\n",
    "**📊 Métricas y reportes:**\n",
    "- Tiempos detallados por fase\n",
    "- Timestamps de inicio y finalización\n",
    "- Resumen de éxito/fallo por componente\n",
    "- Información de conexión a MongoDB\n",
    "\n",
    "#### **🧪 `run_quick_etl_test()` - Función de Pruebas**\n",
    "\n",
    "Versión ligera para validación y desarrollo:\n",
    "- **Propósito**: Probar cambios sin procesar todos los datos\n",
    "- **Por defecto**: Solo procesa 'customers' y 'orders'\n",
    "- **Personalizable**: Permite especificar qué colecciones probar\n",
    "- **Uso típico**: Validar configuraciones antes del ETL completo\n",
    "\n",
    "**💡 Casos de uso:**\n",
    "```python\n",
    "# Prueba básica con 2 colecciones\n",
    "test_success = run_quick_etl_test()\n",
    "\n",
    "# Prueba personalizada con colecciones específicas\n",
    "test_success = run_quick_etl_test(['products', 'payments', 'reviews'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd169fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:26:53,994 - INFO - 🚀 INICIANDO PROCESO ETL COMPLETO CON RETROALIMENTACIÓN MEJORADA\n",
      "2025-08-13 22:26:53,995 - INFO - ======================================================================\n",
      "2025-08-13 22:26:53,996 - INFO - 🔄 Iniciando extracción de datos...\n",
      "2025-08-13 22:26:54,160 - INFO - ✅ customers: 99,441 filas cargadas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 INICIANDO ETL MEJORADO CON MEJOR RETROALIMENTACIÓN\n",
      "============================================================\n",
      "\n",
      "📥 FASE 1: EXTRACCIÓN DE DATOS\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:26:54,896 - INFO - ✅ geolocation: 1,000,163 filas cargadas\n",
      "2025-08-13 22:26:55,141 - INFO - ✅ order_items: 112,650 filas cargadas\n",
      "2025-08-13 22:26:55,245 - INFO - ✅ order_payments: 103,886 filas cargadas\n",
      "2025-08-13 22:26:55,606 - INFO - ✅ order_reviews: 99,224 filas cargadas\n",
      "2025-08-13 22:26:55,986 - INFO - ✅ orders: 99,441 filas cargadas\n",
      "2025-08-13 22:26:56,029 - INFO - ✅ products: 32,951 filas cargadas\n",
      "2025-08-13 22:26:56,035 - INFO - ✅ sellers: 3,095 filas cargadas\n",
      "2025-08-13 22:26:56,038 - INFO - ✅ product_categories: 71 filas cargadas\n",
      "2025-08-13 22:26:56,039 - INFO - 📊 Extracción completada: 9 datasets, 1,550,922 filas totales\n",
      "2025-08-13 22:26:56,040 - INFO - 🔄 Iniciando transformación completa de datos...\n",
      "2025-08-13 22:26:56,040 - INFO - 🔄 Transformando datos de clientes...\n",
      "2025-08-13 22:26:56,156 - INFO - ✅ Clientes transformados: 99,441 registros\n",
      "2025-08-13 22:26:56,157 - INFO - 🔄 Transformando datos de órdenes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracción completada en 2.0s\n",
      "\n",
      "🔧 FASE 2: TRANSFORMACIÓN DE DATOS\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:26:57,624 - INFO - ✅ Órdenes transformadas: 99,441 registros\n",
      "2025-08-13 22:26:57,625 - INFO - 🔄 Transformando datos de productos...\n",
      "2025-08-13 22:26:57,639 - INFO - ✅ Productos transformados: 32,951 registros\n",
      "2025-08-13 22:26:57,641 - INFO - 🔄 Transformando datos de items...\n",
      "2025-08-13 22:26:57,654 - INFO - ✅ Items transformados: 112,650 registros\n",
      "2025-08-13 22:26:57,655 - INFO - 🔄 Transformando datos de pagos...\n",
      "2025-08-13 22:26:57,716 - INFO - ✅ Pagos transformados: 103,886 registros\n",
      "2025-08-13 22:26:57,717 - INFO - 🔄 Transformando datos de reseñas...\n",
      "2025-08-13 22:26:57,884 - INFO - ✅ Reseñas transformadas: 99,224 registros\n",
      "2025-08-13 22:26:57,899 - INFO - ✅ Transformación completa finalizada\n",
      "2025-08-13 22:26:57,900 - INFO - 🚀 Iniciando carga completa a MongoDB con retroalimentación mejorada...\n",
      "2025-08-13 22:26:57,912 - INFO - ✅ Conexión exitosa a MongoDB: olist_ecommerce\n",
      "2025-08-13 22:26:57,913 - INFO - 🔄 Iniciando carga de 99,441 registros a 'customers'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformación completada en 1.9s\n",
      "\n",
      "📤 FASE 3: CARGA A MONGODB CON RETROALIMENTACIÓN MEJORADA\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "🚀 PROCESO DE CARGA MASIVA A MONGODB\n",
      "============================================================\n",
      "📊 Total de colecciones a cargar: 8\n",
      "⏱️ Iniciado: 22:26:57\n",
      "============================================================\n",
      "\n",
      "📋 COLECCIÓN 1/8: CUSTOMERS\n",
      "   📊 Registros a cargar: 99,441\n",
      "   📦 Tamaño de lote: 500\n",
      "   📋 Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:26:58,681 - INFO -    ✅ Colección 'customers' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Datos preparados: 99,441 documentos listos\n",
      "   🗑️ Limpiando colección existente 'customers'...\n",
      "   📦 Dividiendo en 199 lotes de 500 registros\n",
      "   🚀 Iniciando inserción en lotes...\n",
      "   ==================================================\n",
      "   📊 [█░░░░░░░░░░░░░░░░░░░] 9.5% - Lote 19/199 - 9,500/99,441 registros\n",
      "   📊 [███░░░░░░░░░░░░░░░░░] 19.1% - Lote 38/199 - 19,000/99,441 registros\n",
      "   📊 [█████░░░░░░░░░░░░░░░] 28.6% - Lote 57/199 - 28,500/99,441 registros\n",
      "   📊 [███████░░░░░░░░░░░░░] 38.2% - Lote 76/199 - 38,000/99,441 registros\n",
      "   📊 [█████████░░░░░░░░░░░] 47.7% - Lote 95/199 - 47,500/99,441 registros\n",
      "   📊 [███████████░░░░░░░░░] 57.3% - Lote 114/199 - 57,000/99,441 registros\n",
      "   📊 [█████████████░░░░░░░] 66.8% - Lote 133/199 - 66,500/99,441 registros\n",
      "   📊 [███████████████░░░░░] 76.4% - Lote 152/199 - 76,000/99,441 registros\n",
      "   📊 [█████████████████░░░] 85.9% - Lote 171/199 - 85,500/99,441 registros\n",
      "   📊 [███████████████████░] 95.5% - Lote 190/199 - 95,000/99,441 registros\n",
      "   📊 [████████████████████] 100.0% - Lote 199/199 - 99,441/99,441 registros\n",
      "   ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:00,308 - INFO - ✅ Carga completada: 99,441 registros en 'customers'\n",
      "2025-08-13 22:27:00,327 - INFO - 🔄 Iniciando carga de 99,441 registros a 'orders'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Carga completada para 'customers':\n",
      "      📊 Documentos insertados: 99,441\n",
      "      🔍 Documentos verificados: 99,441\n",
      "      📦 Lotes procesados: 199\n",
      "      ❌ Lotes con errores: 0\n",
      "   ✅ Completada en 2.4s (41206 docs/sec)\n",
      "\n",
      "📋 COLECCIÓN 2/8: ORDERS\n",
      "   📊 Registros a cargar: 99,441\n",
      "   📦 Tamaño de lote: 300\n",
      "   📋 Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:01,943 - INFO -    ✅ Colección 'orders' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Datos preparados: 99,441 documentos listos\n",
      "   🗑️ Limpiando colección existente 'orders'...\n",
      "   📦 Dividiendo en 332 lotes de 300 registros\n",
      "   🚀 Iniciando inserción en lotes...\n",
      "   ==================================================\n",
      "   📊 [█░░░░░░░░░░░░░░░░░░░] 9.9% - Lote 33/332 - 9,900/99,441 registros\n",
      "   📊 [███░░░░░░░░░░░░░░░░░] 19.9% - Lote 66/332 - 19,800/99,441 registros\n",
      "   📊 [█████░░░░░░░░░░░░░░░] 29.8% - Lote 99/332 - 29,700/99,441 registros\n",
      "   📊 [███████░░░░░░░░░░░░░] 39.8% - Lote 132/332 - 39,600/99,441 registros\n",
      "   📊 [█████████░░░░░░░░░░░] 49.7% - Lote 165/332 - 49,500/99,441 registros\n",
      "   📊 [███████████░░░░░░░░░] 59.6% - Lote 198/332 - 59,400/99,441 registros\n",
      "   📊 [█████████████░░░░░░░] 69.6% - Lote 231/332 - 69,300/99,441 registros\n",
      "   📊 [███████████████░░░░░] 79.5% - Lote 264/332 - 79,200/99,441 registros\n",
      "   📊 [█████████████████░░░] 89.5% - Lote 297/332 - 89,100/99,441 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:04,561 - INFO - ✅ Carga completada: 99,441 registros en 'orders'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 [███████████████████░] 99.4% - Lote 330/332 - 99,000/99,441 registros\n",
      "   📊 [████████████████████] 100.0% - Lote 332/332 - 99,441/99,441 registros\n",
      "   ==================================================\n",
      "   ✅ Carga completada para 'orders':\n",
      "      📊 Documentos insertados: 99,441\n",
      "      🔍 Documentos verificados: 99,441\n",
      "      📦 Lotes procesados: 332\n",
      "      ❌ Lotes con errores: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:04,712 - INFO - 🔄 Iniciando carga de 32,951 registros a 'products'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Completada en 4.4s (22680 docs/sec)\n",
      "\n",
      "📋 COLECCIÓN 3/8: PRODUCTS\n",
      "   📊 Registros a cargar: 32,951\n",
      "   📦 Tamaño de lote: 500\n",
      "   📋 Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:05,109 - INFO -    ✅ Colección 'products' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Datos preparados: 32,951 documentos listos\n",
      "   🗑️ Limpiando colección existente 'products'...\n",
      "   📦 Dividiendo en 66 lotes de 500 registros\n",
      "   🚀 Iniciando inserción en lotes...\n",
      "   ==================================================\n",
      "   📊 [█░░░░░░░░░░░░░░░░░░░] 9.1% - Lote 6/66 - 3,000/32,951 registros\n",
      "   📊 [███░░░░░░░░░░░░░░░░░] 18.2% - Lote 12/66 - 6,000/32,951 registros\n",
      "   📊 [█████░░░░░░░░░░░░░░░] 27.3% - Lote 18/66 - 9,000/32,951 registros\n",
      "   📊 [███████░░░░░░░░░░░░░] 36.4% - Lote 24/66 - 12,000/32,951 registros\n",
      "   📊 [█████████░░░░░░░░░░░] 45.5% - Lote 30/66 - 15,000/32,951 registros\n",
      "   📊 [██████████░░░░░░░░░░] 54.5% - Lote 36/66 - 18,000/32,951 registros\n",
      "   📊 [████████████░░░░░░░░] 63.6% - Lote 42/66 - 21,000/32,951 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:05,752 - INFO - ✅ Carga completada: 32,951 registros en 'products'\n",
      "2025-08-13 22:27:05,765 - INFO - 🔄 Iniciando carga de 112,650 registros a 'order_items'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 [██████████████░░░░░░] 72.7% - Lote 48/66 - 24,000/32,951 registros\n",
      "   📊 [████████████████░░░░] 81.8% - Lote 54/66 - 27,000/32,951 registros\n",
      "   📊 [██████████████████░░] 90.9% - Lote 60/66 - 30,000/32,951 registros\n",
      "   📊 [████████████████████] 100.0% - Lote 66/66 - 32,951/32,951 registros\n",
      "   ==================================================\n",
      "   ✅ Carga completada para 'products':\n",
      "      📊 Documentos insertados: 32,951\n",
      "      🔍 Documentos verificados: 32,951\n",
      "      📦 Lotes procesados: 66\n",
      "      ❌ Lotes con errores: 0\n",
      "   ✅ Completada en 1.1s (31284 docs/sec)\n",
      "\n",
      "📋 COLECCIÓN 4/8: ORDER_ITEMS\n",
      "   📊 Registros a cargar: 112,650\n",
      "   📦 Tamaño de lote: 200\n",
      "   📋 Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:06,812 - INFO -    ✅ Colección 'order_items' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Datos preparados: 112,650 documentos listos\n",
      "   🗑️ Limpiando colección existente 'order_items'...\n",
      "   📦 Dividiendo en 564 lotes de 200 registros\n",
      "   🚀 Iniciando inserción en lotes...\n",
      "   ==================================================\n",
      "   📊 [█░░░░░░░░░░░░░░░░░░░] 9.9% - Lote 56/564 - 11,200/112,650 registros\n",
      "   📊 [███░░░░░░░░░░░░░░░░░] 19.9% - Lote 112/564 - 22,400/112,650 registros\n",
      "   📊 [█████░░░░░░░░░░░░░░░] 29.8% - Lote 168/564 - 33,600/112,650 registros\n",
      "   📊 [███████░░░░░░░░░░░░░] 39.7% - Lote 224/564 - 44,800/112,650 registros\n",
      "   📊 [█████████░░░░░░░░░░░] 49.6% - Lote 280/564 - 56,000/112,650 registros\n",
      "   📊 [███████████░░░░░░░░░] 59.6% - Lote 336/564 - 67,200/112,650 registros\n",
      "   📊 [█████████████░░░░░░░] 69.5% - Lote 392/564 - 78,400/112,650 registros\n",
      "   📊 [███████████████░░░░░] 79.4% - Lote 448/564 - 89,600/112,650 registros\n",
      "   📊 [█████████████████░░░] 89.4% - Lote 504/564 - 100,800/112,650 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:09,921 - INFO - ✅ Carga completada: 112,650 registros en 'order_items'\n",
      "2025-08-13 22:27:09,970 - INFO - 🔄 Iniciando carga de 103,886 registros a 'payments'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 [███████████████████░] 99.3% - Lote 560/564 - 112,000/112,650 registros\n",
      "   📊 [████████████████████] 100.0% - Lote 564/564 - 112,650/112,650 registros\n",
      "   ==================================================\n",
      "   ✅ Carga completada para 'order_items':\n",
      "      📊 Documentos insertados: 112,650\n",
      "      🔍 Documentos verificados: 112,650\n",
      "      📦 Lotes procesados: 564\n",
      "      ❌ Lotes con errores: 0\n",
      "   ✅ Completada en 4.2s (26790 docs/sec)\n",
      "\n",
      "📋 COLECCIÓN 5/8: PAYMENTS\n",
      "   📊 Registros a cargar: 103,886\n",
      "   📦 Tamaño de lote: 300\n",
      "   📋 Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:10,621 - INFO -    ✅ Colección 'payments' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Datos preparados: 103,886 documentos listos\n",
      "   🗑️ Limpiando colección existente 'payments'...\n",
      "   📦 Dividiendo en 347 lotes de 300 registros\n",
      "   🚀 Iniciando inserción en lotes...\n",
      "   ==================================================\n",
      "   📊 [█░░░░░░░░░░░░░░░░░░░] 9.8% - Lote 34/347 - 10,200/103,886 registros\n",
      "   📊 [███░░░░░░░░░░░░░░░░░] 19.6% - Lote 68/347 - 20,400/103,886 registros\n",
      "   📊 [█████░░░░░░░░░░░░░░░] 29.4% - Lote 102/347 - 30,600/103,886 registros\n",
      "   📊 [███████░░░░░░░░░░░░░] 39.2% - Lote 136/347 - 40,800/103,886 registros\n",
      "   📊 [█████████░░░░░░░░░░░] 49.0% - Lote 170/347 - 51,000/103,886 registros\n",
      "   📊 [███████████░░░░░░░░░] 58.8% - Lote 204/347 - 61,200/103,886 registros\n",
      "   📊 [█████████████░░░░░░░] 68.6% - Lote 238/347 - 71,400/103,886 registros\n",
      "   📊 [███████████████░░░░░] 78.4% - Lote 272/347 - 81,600/103,886 registros\n",
      "   📊 [█████████████████░░░] 88.2% - Lote 306/347 - 91,800/103,886 registros\n",
      "   📊 [███████████████████░] 98.0% - Lote 340/347 - 102,000/103,886 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:12,580 - INFO - ✅ Carga completada: 103,886 registros en 'payments'\n",
      "2025-08-13 22:27:12,597 - INFO - 🔄 Iniciando carga de 99,224 registros a 'reviews'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 [████████████████████] 100.0% - Lote 347/347 - 103,886/103,886 registros\n",
      "   ==================================================\n",
      "   ✅ Carga completada para 'payments':\n",
      "      📊 Documentos insertados: 103,886\n",
      "      🔍 Documentos verificados: 103,886\n",
      "      📦 Lotes procesados: 347\n",
      "      ❌ Lotes con errores: 0\n",
      "   ✅ Completada en 2.6s (39559 docs/sec)\n",
      "\n",
      "📋 COLECCIÓN 6/8: REVIEWS\n",
      "   📊 Registros a cargar: 99,224\n",
      "   📦 Tamaño de lote: 300\n",
      "   📋 Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:13,845 - INFO -    ✅ Colección 'reviews' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Datos preparados: 99,224 documentos listos\n",
      "   🗑️ Limpiando colección existente 'reviews'...\n",
      "   📦 Dividiendo en 331 lotes de 300 registros\n",
      "   🚀 Iniciando inserción en lotes...\n",
      "   ==================================================\n",
      "   📊 [█░░░░░░░░░░░░░░░░░░░] 10.0% - Lote 33/331 - 9,900/99,224 registros\n",
      "   📊 [███░░░░░░░░░░░░░░░░░] 19.9% - Lote 66/331 - 19,800/99,224 registros\n",
      "   📊 [█████░░░░░░░░░░░░░░░] 29.9% - Lote 99/331 - 29,700/99,224 registros\n",
      "   📊 [███████░░░░░░░░░░░░░] 39.9% - Lote 132/331 - 39,600/99,224 registros\n",
      "   📊 [█████████░░░░░░░░░░░] 49.8% - Lote 165/331 - 49,500/99,224 registros\n",
      "   📊 [███████████░░░░░░░░░] 59.8% - Lote 198/331 - 59,400/99,224 registros\n",
      "   📊 [█████████████░░░░░░░] 69.8% - Lote 231/331 - 69,300/99,224 registros\n",
      "   📊 [███████████████░░░░░] 79.8% - Lote 264/331 - 79,200/99,224 registros\n",
      "   📊 [█████████████████░░░] 89.7% - Lote 297/331 - 89,100/99,224 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:16,482 - INFO - ✅ Carga completada: 99,224 registros en 'reviews'\n",
      "2025-08-13 22:27:16,525 - INFO - 🔄 Iniciando carga de 3,095 registros a 'sellers'...\n",
      "2025-08-13 22:27:16,544 - INFO -    ✅ Colección 'sellers' limpiada\n",
      "2025-08-13 22:27:16,608 - INFO - ✅ Carga completada: 3,095 registros en 'sellers'\n",
      "2025-08-13 22:27:16,609 - INFO - 🔄 Iniciando carga de 1,000,163 registros a 'geolocation'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 [███████████████████░] 99.7% - Lote 330/331 - 99,000/99,224 registros\n",
      "   📊 [████████████████████] 100.0% - Lote 331/331 - 99,224/99,224 registros\n",
      "   ==================================================\n",
      "   ✅ Carga completada para 'reviews':\n",
      "      📊 Documentos insertados: 99,224\n",
      "      🔍 Documentos verificados: 99,224\n",
      "      📦 Lotes procesados: 331\n",
      "      ❌ Lotes con errores: 0\n",
      "   ✅ Completada en 3.9s (25260 docs/sec)\n",
      "\n",
      "📋 COLECCIÓN 7/8: SELLERS\n",
      "   📊 Registros a cargar: 3,095\n",
      "   📦 Tamaño de lote: 1,000\n",
      "   📋 Preparando datos para MongoDB...\n",
      "   ✅ Datos preparados: 3,095 documentos listos\n",
      "   🗑️ Limpiando colección existente 'sellers'...\n",
      "   📦 Dividiendo en 4 lotes de 1,000 registros\n",
      "   🚀 Iniciando inserción en lotes...\n",
      "   ==================================================\n",
      "   📊 [█████░░░░░░░░░░░░░░░] 25.0% - Lote 1/4 - 1,000/3,095 registros\n",
      "   📊 [██████████░░░░░░░░░░] 50.0% - Lote 2/4 - 2,000/3,095 registros\n",
      "   📊 [███████████████░░░░░] 75.0% - Lote 3/4 - 3,000/3,095 registros\n",
      "   📊 [████████████████████] 100.0% - Lote 4/4 - 3,095/3,095 registros\n",
      "   ==================================================\n",
      "   ✅ Carga completada para 'sellers':\n",
      "      📊 Documentos insertados: 3,095\n",
      "      🔍 Documentos verificados: 3,095\n",
      "      📦 Lotes procesados: 4\n",
      "      ❌ Lotes con errores: 0\n",
      "   ✅ Completada en 0.1s (36637 docs/sec)\n",
      "\n",
      "📋 COLECCIÓN 8/8: GEOLOCATION\n",
      "   📊 Registros a cargar: 1,000,163\n",
      "   📦 Tamaño de lote: 100\n",
      "   📋 Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:21,495 - INFO -    ✅ Colección 'geolocation' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Datos preparados: 1,000,163 documentos listos\n",
      "   🗑️ Limpiando colección existente 'geolocation'...\n",
      "   📦 Dividiendo en 10002 lotes de 100 registros\n",
      "   🚀 Iniciando inserción en lotes...\n",
      "   ==================================================\n",
      "   📊 [█░░░░░░░░░░░░░░░░░░░] 10.0% - Lote 1000/10002 - 100,000/1,000,163 registros\n",
      "   📊 [███░░░░░░░░░░░░░░░░░] 20.0% - Lote 2000/10002 - 200,000/1,000,163 registros\n",
      "   📊 [█████░░░░░░░░░░░░░░░] 30.0% - Lote 3000/10002 - 300,000/1,000,163 registros\n",
      "   📊 [███████░░░░░░░░░░░░░] 40.0% - Lote 4000/10002 - 400,000/1,000,163 registros\n",
      "   📊 [█████████░░░░░░░░░░░] 50.0% - Lote 5000/10002 - 500,000/1,000,163 registros\n",
      "   📊 [███████████░░░░░░░░░] 60.0% - Lote 6000/10002 - 600,000/1,000,163 registros\n",
      "   📊 [█████████████░░░░░░░] 70.0% - Lote 7000/10002 - 700,000/1,000,163 registros\n",
      "   📊 [███████████████░░░░░] 80.0% - Lote 8000/10002 - 800,000/1,000,163 registros\n",
      "   📊 [█████████████████░░░] 90.0% - Lote 9000/10002 - 900,000/1,000,163 registros\n",
      "   📊 [███████████████████░] 100.0% - Lote 10000/10002 - 1,000,000/1,000,163 registros\n",
      "   📊 [████████████████████] 100.0% - Lote 10002/10002 - 1,000,163/1,000,163 registros\n",
      "   ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:55,557 - INFO - ✅ Carga completada: 1,000,163 registros en 'geolocation'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Carga completada para 'geolocation':\n",
      "      📊 Documentos insertados: 1,000,163\n",
      "      🔍 Documentos verificados: 1,000,163\n",
      "      📦 Lotes procesados: 10002\n",
      "      ❌ Lotes con errores: 0\n",
      "   ✅ Completada en 39.2s (25544 docs/sec)\n",
      "\n",
      "============================================================\n",
      "🔧 CREANDO ÍNDICES PARA OPTIMIZAR CONSULTAS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:55,764 - INFO - 🔄 Creando índices para optimizar consultas...\n",
      "2025-08-13 22:27:58,942 - INFO - ✅ Índices creados exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Índices creados en 3.2s\n",
      "\n",
      "📊 ESTADÍSTICAS FINALES DE CARGA:\n",
      "--------------------------------------------------\n",
      "✅ customers      : 99,441 documentos\n",
      "✅ orders         : 99,441 documentos\n",
      "✅ products       : 32,951 documentos\n",
      "✅ order_items    : 112,650 documentos\n",
      "✅ payments       : 103,886 documentos\n",
      "✅ reviews        : 99,224 documentos\n",
      "✅ sellers        : 3,095 documentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:59,463 - INFO - 🔌 Conexión a MongoDB cerrada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ geolocation    : 1,000,163 documentos\n",
      "--------------------------------------------------\n",
      "📊 TOTAL DOCUMENTOS: 1,550,851\n",
      "\n",
      "🎯 RESUMEN DEL PROCESO:\n",
      "   ✅ Colecciones exitosas: 8/8\n",
      "   📊 Documentos cargados: 1,550,851\n",
      "   ⏱️ Tiempo total: 0:01:01.560159\n",
      "   🚀 Velocidad promedio: 25192 docs/segundo\n",
      "\n",
      "🎉 ¡PROCESO COMPLETADO EXITOSAMENTE!\n",
      "============================================================\n",
      "\n",
      "⏱️ Carga completada en 61.6s\n",
      "\n",
      "======================================================================\n",
      "📊 RESUMEN COMPLETO DEL PROCESO ETL\n",
      "======================================================================\n",
      "⏱️ TIEMPOS DE EJECUCIÓN:\n",
      "   📥 Extracción:        2.0s\n",
      "   🔧 Transformación:    1.9s\n",
      "   📤 Carga:            61.6s\n",
      "   ⏱️ TOTAL:            65.5s\n",
      "\n",
      "📅 TIMESTAMPS:\n",
      "   🚀 Iniciado:   2025-08-13 22:26:53\n",
      "   🏁 Finalizado: 2025-08-13 22:27:59\n",
      "\n",
      "🎉 ¡PROCESO ETL COMPLETADO EXITOSAMENTE!\n",
      "✅ Todos los datos están disponibles en MongoDB\n",
      "🗄️ Base de datos: 'olist_ecommerce'\n",
      "🔗 Host: localhost:27020\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🚀 EJECUTAR ETL COMPLETO CON RETROALIMENTACIÓN MEJORADA\n",
    "print(\"🚀 INICIANDO ETL MEJORADO CON MEJOR RETROALIMENTACIÓN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "etl_success = run_complete_etl_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52c9b7",
   "metadata": {},
   "source": [
    "### ▶️ **Ejecución del ETL Completo**\n",
    "\n",
    "**Esta celda ejecuta todo el proceso ETL mejorado:**\n",
    "\n",
    "Al ejecutar `run_complete_etl_pipeline()` se inicia un proceso automatizado que:\n",
    "\n",
    "1. **🔍 Verifica prerequisitos**: Conexión a MongoDB, archivos CSV disponibles\n",
    "2. **📥 Extrae datos**: Carga todos los datasets del directorio `data/`\n",
    "3. **🔧 Transforma datos**: Aplica limpieza, validaciones y campos derivados\n",
    "4. **📤 Carga a MongoDB**: Inserción optimizada por lotes con retroalimentación\n",
    "5. **🔧 Crea índices**: Optimiza la base de datos para consultas rápidas\n",
    "6. **✅ Valida resultados**: Verifica integridad y genera reporte final\n",
    "\n",
    "**⏱️ Tiempo estimado**: 2-5 minutos dependiendo del hardware y tamaño de datos\n",
    "\n",
    "**📊 Output esperado**:\n",
    "- Progreso detallado por fase\n",
    "- Barras de progreso por colección\n",
    "- Métricas de velocidad (docs/segundo)\n",
    "- Estadísticas finales de carga\n",
    "- Confirmación de éxito/fallo\n",
    "\n",
    "**🚨 Prerequisitos importantes**:\n",
    "- MongoDB debe estar ejecutándose en el puerto configurado (27020)\n",
    "- Los archivos CSV deben estar en el directorio `data/`\n",
    "- Suficiente espacio en disco para la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195fe39",
   "metadata": {},
   "source": [
    "## 5. ✅ VALIDACIÓN Y CONSULTAS DE PRUEBA\n",
    "\n",
    "Funciones para validar que los datos se cargaron correctamente y realizar consultas de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33ae6e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ FUNCIONES DE VALIDACIÓN Y CONSULTAS LISTAS\n",
      "📝 Para usar:\n",
      "   - validate_mongodb_data() : Valida los datos cargados\n",
      "   - run_sample_queries() : Ejecuta consultas de ejemplo\n"
     ]
    }
   ],
   "source": [
    "def validate_mongodb_data():\n",
    "    \"\"\"\n",
    "    Valida que los datos se cargaron correctamente en MongoDB\n",
    "    \"\"\"\n",
    "    logger.info(\"🔍 Validando datos en MongoDB...\")\n",
    "    \n",
    "    client, db = get_mongodb_connection()\n",
    "    \n",
    "    if db is None:\n",
    "        logger.error(\"❌ No se pudo conectar a MongoDB para validación\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        collections = ['customers', 'orders', 'products', 'order_items', \n",
    "                      'payments', 'reviews', 'sellers', 'geolocation']\n",
    "        \n",
    "        print(\"📊 ESTADÍSTICAS DE DATOS EN MONGODB\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        total_documents = 0\n",
    "        \n",
    "        for collection_name in collections:\n",
    "            collection = db[collection_name]\n",
    "            count = collection.count_documents({})\n",
    "            total_documents += count\n",
    "            \n",
    "            # Obtener un documento de ejemplo\n",
    "            sample_doc = collection.find_one()\n",
    "            fields_count = len(sample_doc.keys()) if sample_doc else 0\n",
    "            \n",
    "            print(f\"📋 {collection_name}:\")\n",
    "            print(f\"   📄 Documentos: {count:,}\")\n",
    "            print(f\"   🏷️ Campos: {fields_count}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"📊 TOTAL DE DOCUMENTOS: {total_documents:,}\")\n",
    "        \n",
    "        # Validaciones específicas\n",
    "        print(\"\\\\n🔍 VALIDACIONES ESPECÍFICAS:\")\n",
    "        \n",
    "        # 1. Verificar que existen órdenes con estado 'delivered'\n",
    "        delivered_orders = db.orders.count_documents({'order_status': 'delivered'})\n",
    "        print(f\"✅ Órdenes entregadas: {delivered_orders:,}\")\n",
    "        \n",
    "        # 2. Verificar rangos de fechas\n",
    "        date_range = db.orders.aggregate([\n",
    "            {'$group': {\n",
    "                '_id': None,\n",
    "                'min_date': {'$min': '$order_purchase_timestamp'},\n",
    "                'max_date': {'$max': '$order_purchase_timestamp'}\n",
    "            }}\n",
    "        ])\n",
    "        \n",
    "        for doc in date_range:\n",
    "            print(f\"📅 Rango de fechas: {doc['min_date'].strftime('%Y-%m-%d')} a {doc['max_date'].strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # 3. Verificar integridad referencial básica\n",
    "        order_count = db.orders.count_documents({})\n",
    "        payment_count = db.payments.count_documents({})\n",
    "        print(f\"🔗 Órdenes: {order_count:,}, Pagos: {payment_count:,}\")\n",
    "        \n",
    "        logger.info(\"✅ Validación completada exitosamente\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error en validación: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    finally:\n",
    "        if client:\n",
    "            client.close()\n",
    "\n",
    "def run_sample_queries():\n",
    "    \"\"\"\n",
    "    Ejecuta consultas de ejemplo para demostrar funcionalidad\n",
    "    \"\"\"\n",
    "    logger.info(\"🔍 Ejecutando consultas de ejemplo...\")\n",
    "    \n",
    "    client, db = get_mongodb_connection()\n",
    "    \n",
    "    if db is None:\n",
    "        logger.error(\"❌ No se pudo conectar a MongoDB\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(\"🔍 CONSULTAS DE EJEMPLO EN MONGODB\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # 1. Top 5 estados con más clientes\n",
    "        print(\"1️⃣ Top 5 Estados con Más Clientes:\")\n",
    "        top_states = db.customers.aggregate([\n",
    "            {'$group': {'_id': '$customer_state', 'count': {'$sum': 1}}},\n",
    "            {'$sort': {'count': -1}},\n",
    "            {'$limit': 5}\n",
    "        ])\n",
    "        \n",
    "        for state in top_states:\n",
    "            print(f\"   {state['_id']}: {state['count']:,} clientes\")\n",
    "        \n",
    "        # 2. Promedio de satisfacción por mes\n",
    "        print(\"\\\\n2️⃣ Promedio de Satisfacción por Mes (últimos 6 meses):\")\n",
    "        monthly_satisfaction = db.reviews.aggregate([\n",
    "            {'$match': {'review_score': {'$exists': True}}},\n",
    "            {'$group': {\n",
    "                '_id': {\n",
    "                    'year': {'$year': '$review_creation_date'},\n",
    "                    'month': {'$month': '$review_creation_date'}\n",
    "                },\n",
    "                'avg_score': {'$avg': '$review_score'},\n",
    "                'count': {'$sum': 1}\n",
    "            }},\n",
    "            {'$sort': {'_id.year': -1, '_id.month': -1}},\n",
    "            {'$limit': 6}\n",
    "        ])\n",
    "        \n",
    "        for month in monthly_satisfaction:\n",
    "            year = month['_id']['year']\n",
    "            month_num = month['_id']['month']\n",
    "            avg_score = month['avg_score']\n",
    "            count = month['count']\n",
    "            print(f\"   {year}-{month_num:02d}: {avg_score:.2f} ⭐ ({count} reviews)\")\n",
    "        \n",
    "        # 3. Top 5 categorías por revenue\n",
    "        print(\"\\\\n3️⃣ Top 5 Categorías por Revenue:\")\n",
    "        category_revenue = db.order_items.aggregate([\n",
    "            {'$lookup': {\n",
    "                'from': 'products',\n",
    "                'localField': 'product_id',\n",
    "                'foreignField': 'product_id',\n",
    "                'as': 'product_info'\n",
    "            }},\n",
    "            {'$unwind': '$product_info'},\n",
    "            {'$group': {\n",
    "                '_id': '$product_info.product_category_name_english',\n",
    "                'total_revenue': {'$sum': '$price'},\n",
    "                'total_orders': {'$sum': 1}\n",
    "            }},\n",
    "            {'$sort': {'total_revenue': -1}},\n",
    "            {'$limit': 5}\n",
    "        ])\n",
    "        \n",
    "        for category in category_revenue:\n",
    "            name = category['_id'] or 'Unknown'\n",
    "            revenue = category['total_revenue']\n",
    "            orders = category['total_orders']\n",
    "            print(f\"   {name}: R$ {revenue:,.2f} ({orders:,} items)\")\n",
    "        \n",
    "        # 4. Distribución de métodos de pago\n",
    "        print(\"\\\\n4️⃣ Distribución de Métodos de Pago:\")\n",
    "        payment_distribution = db.payments.aggregate([\n",
    "            {'$group': {\n",
    "                '_id': '$payment_type',\n",
    "                'count': {'$sum': 1},\n",
    "                'total_value': {'$sum': '$payment_value'}\n",
    "            }},\n",
    "            {'$sort': {'count': -1}}\n",
    "        ])\n",
    "        \n",
    "        for payment in payment_distribution:\n",
    "            method = payment['_id']\n",
    "            count = payment['count']\n",
    "            value = payment['total_value']\n",
    "            print(f\"   {method}: {count:,} transacciones (R$ {value:,.2f})\")\n",
    "        \n",
    "        logger.info(\"✅ Consultas de ejemplo completadas\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error ejecutando consultas: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        if client:\n",
    "            client.close()\n",
    "\n",
    "# Funciones de utilidad para consultas\n",
    "print(\"🛠️ FUNCIONES DE VALIDACIÓN Y CONSULTAS LISTAS\")\n",
    "print(\"📝 Para usar:\")\n",
    "print(\"   - validate_mongodb_data() : Valida los datos cargados\")\n",
    "print(\"   - run_sample_queries() : Ejecuta consultas de ejemplo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03dab5",
   "metadata": {},
   "source": [
    "### ✅ **Funciones de Validación y Consultas de Ejemplo**\n",
    "\n",
    "Esta sección proporciona herramientas para validar y explorar los datos cargados en MongoDB:\n",
    "\n",
    "#### **🔍 `validate_mongodb_data()` - Validación de Integridad**\n",
    "\n",
    "Función completa de validación post-ETL que verifica:\n",
    "\n",
    "**📊 Estadísticas básicas:**\n",
    "- Conteo de documentos por colección\n",
    "- Número de campos por tipo de documento\n",
    "- Total de documentos en la base de datos\n",
    "\n",
    "**✅ Validaciones específicas:**\n",
    "- **Órdenes entregadas**: Verifica que existen órdenes con status 'delivered'\n",
    "- **Rangos de fechas**: Valida que las fechas están en rangos esperados\n",
    "- **Integridad referencial**: Compara conteos entre colecciones relacionadas\n",
    "- **Consistencia de datos**: Verifica que no hay datos corruptos\n",
    "\n",
    "#### **🔍 `run_sample_queries()` - Consultas Demostrativas**\n",
    "\n",
    "Ejecuta consultas representativas para demostrar la funcionalidad de MongoDB:\n",
    "\n",
    "**📈 Consultas implementadas:**\n",
    "\n",
    "1. **Top 5 Estados por Clientes**: Análisis geográfico de la base de clientes\n",
    "2. **Satisfacción Temporal**: Promedio de satisfacción por mes (últimos 6 meses)\n",
    "3. **Revenue por Categoría**: Top 5 categorías de productos por ingresos\n",
    "4. **Métodos de Pago**: Distribución de tipos de pago y valores\n",
    "\n",
    "**🎯 Propósito de las consultas:**\n",
    "- **Validar joins**: Verificar que las relaciones entre colecciones funcionan\n",
    "- **Probar agregaciones**: Confirmar que MongoDB puede realizar cálculos complejos\n",
    "- **Demostrar capacidades**: Mostrar el tipo de análisis posibles\n",
    "- **Benchmark**: Medir performance de consultas con índices\n",
    "\n",
    "**📝 Uso recomendado:**\n",
    "```python\n",
    "# Después del ETL, validar la carga\n",
    "validate_mongodb_data()\n",
    "\n",
    "# Explorar datos con consultas de ejemplo\n",
    "run_sample_queries()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0ee0e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:32:32,291 - INFO - 🔍 Validando datos en MongoDB...\n",
      "2025-08-13 22:32:32,315 - INFO - ✅ Conexión exitosa a MongoDB: olist_ecommerce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 ESTADÍSTICAS DE DATOS EN MONGODB\n",
      "==================================================\n",
      "📋 customers:\n",
      "   📄 Documentos: 99,441\n",
      "   🏷️ Campos: 7\n",
      "\n",
      "📋 orders:\n",
      "   📄 Documentos: 99,441\n",
      "   🏷️ Campos: 16\n",
      "\n",
      "📋 products:\n",
      "   📄 Documentos: 32,951\n",
      "   🏷️ Campos: 13\n",
      "\n",
      "📋 order_items:\n",
      "   📄 Documentos: 112,650\n",
      "   🏷️ Campos: 11\n",
      "\n",
      "📋 payments:\n",
      "   📄 Documentos: 103,886\n",
      "   🏷️ Campos: 7\n",
      "\n",
      "📋 reviews:\n",
      "   📄 Documentos: 99,224\n",
      "   🏷️ Campos: 12\n",
      "\n",
      "📋 sellers:\n",
      "   📄 Documentos: 3,095\n",
      "   🏷️ Campos: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:32:33,014 - INFO - ✅ Validación completada exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 geolocation:\n",
      "   📄 Documentos: 1,000,163\n",
      "   🏷️ Campos: 6\n",
      "\n",
      "📊 TOTAL DE DOCUMENTOS: 1,550,851\n",
      "\\n🔍 VALIDACIONES ESPECÍFICAS:\n",
      "✅ Órdenes entregadas: 96,478\n",
      "📅 Rango de fechas: 2016-09-04 a 2018-10-17\n",
      "🔗 Órdenes: 99,441, Pagos: 103,886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_mongodb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f711d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:32:43,796 - INFO - 🔍 Ejecutando consultas de ejemplo...\n",
      "2025-08-13 22:32:43,807 - INFO - ✅ Conexión exitosa a MongoDB: olist_ecommerce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 CONSULTAS DE EJEMPLO EN MONGODB\n",
      "==================================================\n",
      "1️⃣ Top 5 Estados con Más Clientes:\n",
      "   SP: 41,746 clientes\n",
      "   RJ: 12,852 clientes\n",
      "   MG: 11,635 clientes\n",
      "   RS: 5,466 clientes\n",
      "   PR: 5,045 clientes\n",
      "\\n2️⃣ Promedio de Satisfacción por Mes (últimos 6 meses):\n",
      "   2018-08: 4.21 ⭐ (8987 reviews)\n",
      "   2018-07: 4.29 ⭐ (5634 reviews)\n",
      "   2018-06: 4.20 ⭐ (6715 reviews)\n",
      "   2018-05: 4.19 ⭐ (7458 reviews)\n",
      "   2018-04: 3.92 ⭐ (7287 reviews)\n",
      "   2018-03: 3.73 ⭐ (7803 reviews)\n",
      "\\n3️⃣ Top 5 Categorías por Revenue:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_sample_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mrun_sample_queries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# 3. Top 5 categorías por revenue\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn3️⃣ Top 5 Categorías por Revenue:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m category_revenue = \u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43morder_items\u001b[49m\u001b[43m.\u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$lookup\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfrom\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproducts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocalField\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mforeignField\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mas\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_info\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$unwind\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$product_info\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$group\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$product_info.product_category_name_english\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_revenue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$sum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$price\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_orders\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$sum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$sort\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_revenue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$limit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m category_revenue:\n\u001b[32m    143\u001b[39m     name = category[\u001b[33m'\u001b[39m\u001b[33m_id\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mUnknown\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\collection.py:2979\u001b[39m, in \u001b[36mCollection.aggregate\u001b[39m\u001b[34m(self, pipeline, session, let, comment, **kwargs)\u001b[39m\n\u001b[32m   2902\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Perform an aggregation using the aggregation framework on this\u001b[39;00m\n\u001b[32m   2903\u001b[39m \u001b[33;03mcollection.\u001b[39;00m\n\u001b[32m   2904\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2976\u001b[39m \u001b[33;03m    https://mongodb.com/docs/manual/reference/command/aggregate\u001b[39;00m\n\u001b[32m   2977\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2978\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._database.client._tmp_session(session, close=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[32m-> \u001b[39m\u001b[32m2979\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_aggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_CollectionAggregationCommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCommandCursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexplicit_session\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2987\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2988\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\_csot.py:125\u001b[39m, in \u001b[36mapply.<locals>.csot_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\collection.py:2886\u001b[39m, in \u001b[36mCollection._aggregate\u001b[39m\u001b[34m(self, aggregation_command, pipeline, cursor_class, session, explicit_session, let, comment, **kwargs)\u001b[39m\n\u001b[32m   2875\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcomment\u001b[39m\u001b[33m\"\u001b[39m] = comment\n\u001b[32m   2876\u001b[39m cmd = aggregation_command(\n\u001b[32m   2877\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2878\u001b[39m     cursor_class,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2883\u001b[39m     user_fields={\u001b[33m\"\u001b[39m\u001b[33mcursor\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mfirstBatch\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m}},\n\u001b[32m   2884\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2886\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_database\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_retryable_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_cursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_read_preference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_performs_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2891\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_Op\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAGGREGATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2892\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:2026\u001b[39m, in \u001b[36mMongoClient._retryable_read\u001b[39m\u001b[34m(self, func, read_pref, session, operation, address, retryable, operation_id)\u001b[39m\n\u001b[32m   2021\u001b[39m \u001b[38;5;66;03m# Ensure that the client supports retrying on reads and there is no session in\u001b[39;00m\n\u001b[32m   2022\u001b[39m \u001b[38;5;66;03m# transaction, otherwise, we will not support retry behavior for this call.\u001b[39;00m\n\u001b[32m   2023\u001b[39m retryable = \u001b[38;5;28mbool\u001b[39m(\n\u001b[32m   2024\u001b[39m     retryable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.options.retry_reads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (session \u001b[38;5;129;01mand\u001b[39;00m session.in_transaction)\n\u001b[32m   2025\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_read\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m=\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\_csot.py:125\u001b[39m, in \u001b[36mapply.<locals>.csot_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:1993\u001b[39m, in \u001b[36mMongoClient._retry_internal\u001b[39m\u001b[34m(self, func, session, bulk, operation, is_read, address, read_pref, retryable, operation_id)\u001b[39m\n\u001b[32m   1956\u001b[39m \u001b[38;5;129m@_csot\u001b[39m.apply\n\u001b[32m   1957\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retry_internal\u001b[39m(\n\u001b[32m   1958\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1967\u001b[39m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1968\u001b[39m ) -> T:\n\u001b[32m   1969\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Internal retryable helper for all client transactions.\u001b[39;00m\n\u001b[32m   1970\u001b[39m \n\u001b[32m   1971\u001b[39m \u001b[33;03m    :param func: Callback function we want to retry\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1980\u001b[39m \u001b[33;03m    :return: Output of the calling func()\u001b[39;00m\n\u001b[32m   1981\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1982\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ClientConnectionRetryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmongo_client\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbulk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1986\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_read\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m=\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1992\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1993\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:2730\u001b[39m, in \u001b[36m_ClientConnectionRetryable.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2728\u001b[39m \u001b[38;5;28mself\u001b[39m._check_last_error(check_csot=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2729\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_read \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write()\n\u001b[32m   2731\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[32m   2732\u001b[39m     \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[32m   2733\u001b[39m     \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[32m   2734\u001b[39m     \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n\u001b[32m   2735\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_last_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:2891\u001b[39m, in \u001b[36m_ClientConnectionRetryable._read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrying:\n\u001b[32m   2884\u001b[39m     _debug_log(\n\u001b[32m   2885\u001b[39m         _COMMAND_LOGGER,\n\u001b[32m   2886\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrying read attempt number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._attempt_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2889\u001b[39m         operationId=\u001b[38;5;28mself\u001b[39m._operation_id,\n\u001b[32m   2890\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2891\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_server\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\aggregation.py:164\u001b[39m, in \u001b[36m_AggregationCommand.get_cursor\u001b[39m\u001b[34m(self, session, server, conn, read_preference)\u001b[39m\n\u001b[32m    161\u001b[39m     write_concern = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Run command.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m result = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_database\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_target\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_concern\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_database\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_user_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result_processor:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._result_processor(result, conn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\helpers.py:47\u001b[39m, in \u001b[36m_handle_reauth.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpymongo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msynchronous\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Connection\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailure \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m no_reauth:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\pool.py:442\u001b[39m, in \u001b[36mConnection.command\u001b[39m\u001b[34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Catch socket.error, KeyboardInterrupt, CancelledError, etc. and close ourselves.\u001b[39;00m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_connection_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\pool.py:414\u001b[39m, in \u001b[36mConnection.command\u001b[39m\u001b[34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[39m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_if_not_writable(unacknowledged)\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_mongos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowable_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlisteners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_bson_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression_ctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_op_msg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mop_msg_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43munacknowledged\u001b[49m\u001b[43m=\u001b[49m\u001b[43munacknowledged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexhaust_allowed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexhaust_allowed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (OperationFailure, NotPrimaryError):\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\network.py:198\u001b[39m, in \u001b[36mcommand\u001b[39m\u001b[34m(conn, dbname, spec, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields, exhaust_allowed, write_concern)\u001b[39m\n\u001b[32m    196\u001b[39m     response_doc: _DocumentOut = {\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m}\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     reply = \u001b[43mreceive_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     conn.more_to_come = reply.more_to_come\n\u001b[32m    200\u001b[39m     unpacked_docs = reply.unpack_response(\n\u001b[32m    201\u001b[39m         codec_options=codec_options, user_fields=user_fields\n\u001b[32m    202\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\network_layer.py:759\u001b[39m, in \u001b[36mreceive_message\u001b[39m\u001b[34m(conn, request_id, max_message_size)\u001b[39m\n\u001b[32m    757\u001b[39m         deadline = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    758\u001b[39m \u001b[38;5;66;03m# Ignore the response's request id.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m length, _, response_to, op_code = _UNPACK_HEADER(\u001b[43mreceive_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# No request_id for exhaust cursor \"getMore\".\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\network_layer.py:343\u001b[39m, in \u001b[36mreceive_data\u001b[39m\u001b[34m(conn, length, deadline)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    340\u001b[39m     \u001b[38;5;66;03m# Use the legacy wait_for_read cancellation approach on PyPy due to PYTHON-5011.\u001b[39;00m\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# also use it on Windows due to PYTHON-5405\u001b[39;00m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _PYPY \u001b[38;5;129;01mor\u001b[39;00m _WINDOWS:\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m         \u001b[43mwait_for_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _csot.get_timeout() \u001b[38;5;129;01mand\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    345\u001b[39m             conn.set_conn_timeout(\u001b[38;5;28mmax\u001b[39m(deadline - time.monotonic(), \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\network_layer.py:316\u001b[39m, in \u001b[36mwait_for_read\u001b[39m\u001b[34m(conn, deadline)\u001b[39m\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    315\u001b[39m         timeout = _POLL_TIMEOUT\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     readable = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msocket_checker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.cancel_context.cancelled:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _OperationCancelled(\u001b[33m\"\u001b[39m\u001b[33moperation cancelled\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\socket_checker.py:77\u001b[39m, in \u001b[36mSocketChecker.select\u001b[39m\u001b[34m(self, sock, read, write, timeout)\u001b[39m\n\u001b[32m     75\u001b[39m rlist = [sock] \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m     76\u001b[39m wlist = [sock] \u001b[38;5;28;01mif\u001b[39;00m write \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m res = \u001b[43mselect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43msock\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# select returns a 3-tuple of lists of objects that are\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# ready: subsets of the first three arguments. Return\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# True if any of the lists are not empty.\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(res)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_sample_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be232dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
