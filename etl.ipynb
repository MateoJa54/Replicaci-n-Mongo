{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e21ee56d",
   "metadata": {},
   "source": [
    "# üîÑ Proceso ETL - Dataset Olist hacia MongoDB\n",
    "\n",
    "## Extracci√≥n, Transformaci√≥n y Carga de datos de e-commerce brasile√±o\n",
    "\n",
    "Este notebook implementa un proceso ETL completo para:\n",
    "1. **Extraer** datos de archivos CSV del dataset Olist\n",
    "2. **Transformar** y limpiar los datos para an√°lisis\n",
    "3. **Cargar** los datos procesados en MongoDB\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75af7fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas exitosamente\n",
      "üìÖ Proceso ETL iniciado: 2025-08-13 22:25:52\n"
     ]
    }
   ],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "\n",
    "# Configuraciones\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas exitosamente\")\n",
    "print(f\"üìÖ Proceso ETL iniciado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec60fa9",
   "metadata": {},
   "source": [
    "### üìö **Importaci√≥n de Librer√≠as**\n",
    "\n",
    "Esta celda configura todas las dependencias necesarias para el proceso ETL:\n",
    "\n",
    "- **pandas & numpy**: Manipulaci√≥n y an√°lisis de datos\n",
    "- **pymongo**: Cliente oficial de MongoDB para Python\n",
    "- **datetime**: Manejo de fechas y timestamps\n",
    "- **logging**: Sistema de registro de eventos y errores\n",
    "- **typing**: Anotaciones de tipos para mejor documentaci√≥n del c√≥digo\n",
    "\n",
    "**Configuraciones importantes:**\n",
    "- Se suprimen las advertencias para output m√°s limpio\n",
    "- Se configura pandas para mostrar todas las columnas\n",
    "- Se establece logging con formato timestamp para trazabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb7c71",
   "metadata": {},
   "source": [
    "## 1. üì• EXTRACCI√ìN (Extract)\n",
    "\n",
    "Carga de datos desde archivos CSV del dataset Olist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56672175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:25:55,481 - INFO - üîÑ Iniciando extracci√≥n de datos...\n",
      "2025-08-13 22:25:55,646 - INFO - ‚úÖ customers: 99,441 filas cargadas\n",
      "2025-08-13 22:25:56,272 - INFO - ‚úÖ geolocation: 1,000,163 filas cargadas\n",
      "2025-08-13 22:25:56,530 - INFO - ‚úÖ order_items: 112,650 filas cargadas\n",
      "2025-08-13 22:25:56,636 - INFO - ‚úÖ order_payments: 103,886 filas cargadas\n",
      "2025-08-13 22:25:56,991 - INFO - ‚úÖ order_reviews: 99,224 filas cargadas\n",
      "2025-08-13 22:25:57,438 - INFO - ‚úÖ orders: 99,441 filas cargadas\n",
      "2025-08-13 22:25:57,480 - INFO - ‚úÖ products: 32,951 filas cargadas\n",
      "2025-08-13 22:25:57,486 - INFO - ‚úÖ sellers: 3,095 filas cargadas\n",
      "2025-08-13 22:25:57,488 - INFO - ‚úÖ product_categories: 71 filas cargadas\n",
      "2025-08-13 22:25:57,489 - INFO - üìä Extracci√≥n completada: 9 datasets, 1,550,922 filas totales\n"
     ]
    }
   ],
   "source": [
    "def extract_data() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Extrae todos los archivos CSV del dataset Olist\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Diccionario con todos los datasets cargados\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÑ Iniciando extracci√≥n de datos...\")\n",
    "    \n",
    "    # Definir archivos y sus nombres de colecci√≥n en MongoDB\n",
    "    files_config = {\n",
    "        'customers': 'olist_customers_dataset.csv',\n",
    "        'geolocation': 'olist_geolocation_dataset.csv',\n",
    "        'order_items': 'olist_order_items_dataset.csv',\n",
    "        'order_payments': 'olist_order_payments_dataset.csv',\n",
    "        'order_reviews': 'olist_order_reviews_dataset.csv',\n",
    "        'orders': 'olist_orders_dataset.csv',\n",
    "        'products': 'olist_products_dataset.csv',\n",
    "        'sellers': 'olist_sellers_dataset.csv',\n",
    "        'product_categories': 'product_category_name_translation.csv'\n",
    "    }\n",
    "    \n",
    "    datasets = {}\n",
    "    total_rows = 0\n",
    "    \n",
    "    for dataset_name, filename in files_config.items():\n",
    "        try:\n",
    "            file_path = f'data/{filename}'\n",
    "            df = pd.read_csv(file_path)\n",
    "            datasets[dataset_name] = df\n",
    "            total_rows += len(df)\n",
    "            logger.info(f\"‚úÖ {dataset_name}: {len(df):,} filas cargadas\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"‚ùå Archivo no encontrado: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error cargando {filename}: {str(e)}\")\n",
    "    \n",
    "    logger.info(f\"üìä Extracci√≥n completada: {len(datasets)} datasets, {total_rows:,} filas totales\")\n",
    "    return datasets\n",
    "\n",
    "# Ejecutar extracci√≥n\n",
    "raw_data = extract_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ea951",
   "metadata": {},
   "source": [
    "### üì• **Funci√≥n de Extracci√≥n de Datos**\n",
    "\n",
    "**`extract_data()` - Carga autom√°tica de datasets CSV**\n",
    "\n",
    "Esta funci√≥n implementa la fase **Extract** del proceso ETL:\n",
    "\n",
    "**üîß Funcionalidades:**\n",
    "- **Carga autom√°tica**: Lee todos los archivos CSV del dataset Olist\n",
    "- **Mapeo de archivos**: Asocia cada CSV con su nombre l√≥gico en el sistema\n",
    "- **Manejo de errores**: Contin√∫a el proceso aunque falten algunos archivos\n",
    "- **Logging detallado**: Registra el progreso y estad√≠sticas de cada archivo\n",
    "- **Validaci√≥n**: Verifica que los archivos existan antes de cargarlos\n",
    "\n",
    "**üìä Archivos procesados:**\n",
    "- `customers` - Datos de clientes (ubicaci√≥n, identificaci√≥n)\n",
    "- `orders` - Informaci√≥n de √≥rdenes y estados\n",
    "- `products` - Cat√°logo de productos y categor√≠as\n",
    "- `order_items` - Items individuales de cada orden\n",
    "- `payments` - Informaci√≥n de pagos y m√©todos\n",
    "- `reviews` - Rese√±as y calificaciones de clientes\n",
    "- `sellers` - Datos de vendedores\n",
    "- `geolocation` - Informaci√≥n geogr√°fica detallada\n",
    "- `product_categories` - Traducci√≥n de categor√≠as al ingl√©s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb27af",
   "metadata": {},
   "source": [
    "## 2. üîß TRANSFORMACI√ìN (Transform)\n",
    "\n",
    "Limpieza, validaci√≥n y transformaci√≥n de datos para optimizar el almacenamiento en MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e242adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dates(df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convierte columnas de fecha a formato datetime y maneja valores nulos\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in df_transformed.columns:\n",
    "            df_transformed[col] = pd.to_datetime(df_transformed[col], errors='coerce')\n",
    "            \n",
    "    return df_transformed\n",
    "\n",
    "def transform_customers(customers_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de clientes\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÑ Transformando datos de clientes...\")\n",
    "    \n",
    "    df = customers_df.copy()\n",
    "    \n",
    "    # Limpiar datos de ubicaci√≥n\n",
    "    df['customer_city'] = df['customer_city'].str.lower().str.strip()\n",
    "    df['customer_state'] = df['customer_state'].str.upper().str.strip()\n",
    "    \n",
    "    # Agregar campos derivados\n",
    "    df['customer_location'] = df['customer_city'] + ', ' + df['customer_state']\n",
    "    \n",
    "    # Validar c√≥digos postales (CEP brasile√±o)\n",
    "    df['customer_zip_code_prefix'] = df['customer_zip_code_prefix'].astype(str).str.zfill(5)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Clientes transformados: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "def transform_orders(orders_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de √≥rdenes\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÑ Transformando datos de √≥rdenes...\")\n",
    "    \n",
    "    df = orders_df.copy()\n",
    "    \n",
    "    # Convertir fechas\n",
    "    date_columns = ['order_purchase_timestamp', 'order_approved_at', \n",
    "                   'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "                   'order_estimated_delivery_date']\n",
    "    \n",
    "    df = transform_dates(df, date_columns)\n",
    "    \n",
    "    # Agregar campos derivados\n",
    "    df['order_year'] = df['order_purchase_timestamp'].dt.year\n",
    "    df['order_month'] = df['order_purchase_timestamp'].dt.month\n",
    "    df['order_weekday'] = df['order_purchase_timestamp'].dt.dayofweek\n",
    "    df['order_hour'] = df['order_purchase_timestamp'].dt.hour\n",
    "    \n",
    "    # Calcular tiempos de entrega\n",
    "    df['days_to_deliver'] = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.days\n",
    "    df['days_vs_estimated'] = (df['order_delivered_customer_date'] - df['order_estimated_delivery_date']).dt.days\n",
    "    \n",
    "    # Categorizar estado de entrega\n",
    "    df['delivery_status'] = df.apply(lambda row: \n",
    "        'on_time' if pd.notna(row['days_vs_estimated']) and row['days_vs_estimated'] <= 0\n",
    "        else 'late' if pd.notna(row['days_vs_estimated']) and row['days_vs_estimated'] > 0\n",
    "        else 'unknown', axis=1)\n",
    "    \n",
    "    logger.info(f\"‚úÖ √ìrdenes transformadas: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "def transform_products(products_df: pd.DataFrame, categories_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de productos y a√±ade categor√≠as en ingl√©s\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÑ Transformando datos de productos...\")\n",
    "    \n",
    "    df = products_df.copy()\n",
    "    \n",
    "    # Unir con traducciones de categor√≠as\n",
    "    df = df.merge(categories_df, on='product_category_name', how='left')\n",
    "    \n",
    "    # Limpiar y estandarizar categor√≠as\n",
    "    df['product_category_name'] = df['product_category_name'].fillna('unknown')\n",
    "    df['product_category_name_english'] = df['product_category_name_english'].fillna('unknown')\n",
    "    \n",
    "    # Calcular volumen del producto\n",
    "    df['product_volume_cm3'] = (df['product_length_cm'] * \n",
    "                               df['product_height_cm'] * \n",
    "                               df['product_width_cm'])\n",
    "    \n",
    "    # Categorizar tama√±o por peso\n",
    "    df['weight_category'] = pd.cut(df['product_weight_g'], \n",
    "                                  bins=[0, 500, 2000, 10000, float('inf')],\n",
    "                                  labels=['light', 'medium', 'heavy', 'very_heavy'])\n",
    "    \n",
    "    logger.info(f\"‚úÖ Productos transformados: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "def transform_order_items(order_items_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de items de √≥rdenes\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÑ Transformando datos de items...\")\n",
    "    \n",
    "    df = order_items_df.copy()\n",
    "    \n",
    "    # Calcular m√©tricas de precio\n",
    "    df['unit_price'] = df['price'] / df['order_item_id']  # Precio por unidad\n",
    "    df['total_item_value'] = df['price'] + df['freight_value']  # Valor total con env√≠o\n",
    "    \n",
    "    # Categorizar valor del env√≠o\n",
    "    df['freight_category'] = pd.cut(df['freight_value'], \n",
    "                                   bins=[0, 10, 30, 100, float('inf')],\n",
    "                                   labels=['low', 'medium', 'high', 'very_high'])\n",
    "    \n",
    "    logger.info(f\"‚úÖ Items transformados: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "def transform_payments(payments_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de pagos\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÑ Transformando datos de pagos...\")\n",
    "    \n",
    "    df = payments_df.copy()\n",
    "    \n",
    "    # Categorizar valores de pago\n",
    "    df['payment_range'] = pd.cut(df['payment_value'], \n",
    "                                bins=[0, 50, 100, 200, 500, float('inf')],\n",
    "                                labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "    \n",
    "    # Normalizar tipos de pago\n",
    "    df['payment_type'] = df['payment_type'].str.lower().str.replace('_', ' ')\n",
    "    \n",
    "    logger.info(f\"‚úÖ Pagos transformados: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "def transform_reviews(reviews_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforma dataset de rese√±as\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÑ Transformando datos de rese√±as...\")\n",
    "    \n",
    "    df = reviews_df.copy()\n",
    "    \n",
    "    # Convertir fechas\n",
    "    date_columns = ['review_creation_date', 'review_answer_timestamp']\n",
    "    df = transform_dates(df, date_columns)\n",
    "    \n",
    "    # Categorizar satisfacci√≥n\n",
    "    df['satisfaction_level'] = df['review_score'].map({\n",
    "        1: 'very_dissatisfied',\n",
    "        2: 'dissatisfied', \n",
    "        3: 'neutral',\n",
    "        4: 'satisfied',\n",
    "        5: 'very_satisfied'\n",
    "    })\n",
    "    \n",
    "    # Analizar longitud de comentarios\n",
    "    df['comment_title_length'] = df['review_comment_title'].str.len().fillna(0)\n",
    "    df['comment_message_length'] = df['review_comment_message'].str.len().fillna(0)\n",
    "    df['has_comment'] = (df['comment_title_length'] + df['comment_message_length']) > 0\n",
    "    \n",
    "    logger.info(f\"‚úÖ Rese√±as transformadas: {len(df):,} registros\")\n",
    "    return df\n",
    "\n",
    "# Aplicar todas las transformaciones\n",
    "def transform_all_data(raw_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aplica todas las transformaciones a los datasets\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÑ Iniciando transformaci√≥n completa de datos...\")\n",
    "    \n",
    "    transformed_data = {}\n",
    "    \n",
    "    # Transformar cada dataset\n",
    "    transformed_data['customers'] = transform_customers(raw_data['customers'])\n",
    "    transformed_data['orders'] = transform_orders(raw_data['orders'])\n",
    "    transformed_data['products'] = transform_products(raw_data['products'], raw_data['product_categories'])\n",
    "    transformed_data['order_items'] = transform_order_items(raw_data['order_items'])\n",
    "    transformed_data['payments'] = transform_payments(raw_data['order_payments'])\n",
    "    transformed_data['reviews'] = transform_reviews(raw_data['order_reviews'])\n",
    "    \n",
    "    # Datasets que no requieren transformaci√≥n especial\n",
    "    transformed_data['sellers'] = raw_data['sellers'].copy()\n",
    "    transformed_data['geolocation'] = raw_data['geolocation'].copy()\n",
    "    \n",
    "    logger.info(\"‚úÖ Transformaci√≥n completa finalizada\")\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419cc4c",
   "metadata": {},
   "source": [
    "### üîß **Funciones de Transformaci√≥n de Datos**\n",
    "\n",
    "Esta secci√≥n contiene todas las funciones especializadas para la fase **Transform** del ETL:\n",
    "\n",
    "#### **üîÑ `transform_dates()`** \n",
    "Convierte columnas de texto a formato datetime, manejando valores nulos y errores de formato.\n",
    "\n",
    "#### **üë• `transform_customers()`**\n",
    "- Estandariza nombres de ciudades (min√∫sculas) y estados (may√∫sculas)\n",
    "- Crea campo combinado de ubicaci√≥n\n",
    "- Valida y formatea c√≥digos postales brasile√±os (CEP)\n",
    "- Agrega campos derivados para an√°lisis geogr√°fico\n",
    "\n",
    "#### **üì¶ `transform_orders()`**\n",
    "- Convierte timestamps de √≥rdenes a formato datetime\n",
    "- Extrae componentes temporales (a√±o, mes, d√≠a de semana, hora)\n",
    "- Calcula m√©tricas de entrega (d√≠as de entrega, comparaci√≥n con estimado)\n",
    "- Categoriza el estado de entrega (a tiempo, tard√≠o, desconocido)\n",
    "\n",
    "#### **üõçÔ∏è `transform_products()`**\n",
    "- Une productos con traducciones de categor√≠as al ingl√©s\n",
    "- Calcula volumen del producto (cm¬≥)\n",
    "- Categoriza productos por peso (ligero, medio, pesado, muy pesado)\n",
    "- Estandariza nombres de categor√≠as\n",
    "\n",
    "#### **üìã `transform_order_items()`**\n",
    "- Calcula precio unitario y valor total con env√≠o\n",
    "- Categoriza costos de env√≠o por rangos\n",
    "- Agrega m√©tricas de valor por item\n",
    "\n",
    "#### **üí≥ `transform_payments()`**\n",
    "- Categoriza valores de pago por rangos\n",
    "- Normaliza tipos de pago\n",
    "- Estandariza formato de m√©todos de pago\n",
    "\n",
    "#### **‚≠ê `transform_reviews()`**\n",
    "- Convierte fechas de rese√±as\n",
    "- Mapea scores num√©ricos a niveles de satisfacci√≥n\n",
    "- Analiza longitud de comentarios\n",
    "- Detecta presencia de comentarios textuales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d593d3",
   "metadata": {},
   "source": [
    "## 3. üì§ CARGA (Load) - Configuraci√≥n MongoDB\n",
    "\n",
    "Configuraci√≥n de conexi√≥n y carga de datos a MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ce5fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Configurando conexi√≥n a MongoDB...\n",
      "üìç Host: localhost:27020\n",
      "üóÑÔ∏è Base de datos: olist_ecommerce\n",
      "\n",
      "‚ö†Ô∏è  NOTA: Aseg√∫rate de que MongoDB est√© ejecut√°ndose antes de continuar\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n de MongoDB\n",
    "MONGODB_CONFIG = {\n",
    "    'host': 'localhost',  # Cambiar por tu host de MongoDB\n",
    "    'port': 27020,        # Puerto por defecto de MongoDB\n",
    "    'database': 'olist_ecommerce',  # Nombre de la base de datos\n",
    "    'username': None,     # Agregar si usas autenticaci√≥n\n",
    "    'password': None      # Agregar si usas autenticaci√≥n\n",
    "}\n",
    "\n",
    "def get_mongodb_connection():\n",
    "    \"\"\"\n",
    "    Establece conexi√≥n con MongoDB\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construir URI de conexi√≥n\n",
    "        if MONGODB_CONFIG['username'] and MONGODB_CONFIG['password']:\n",
    "            connection_string = f\"mongodb://{MONGODB_CONFIG['username']}:{MONGODB_CONFIG['password']}@{MONGODB_CONFIG['host']}:{MONGODB_CONFIG['port']}/{MONGODB_CONFIG['database']}\"\n",
    "        else:\n",
    "            connection_string = f\"mongodb://{MONGODB_CONFIG['host']}:{MONGODB_CONFIG['port']}/\"\n",
    "        \n",
    "        # Conectar a MongoDB\n",
    "        client = MongoClient(connection_string)\n",
    "        \n",
    "        # Verificar conexi√≥n\n",
    "        client.admin.command('ping')\n",
    "        \n",
    "        # Obtener base de datos\n",
    "        db = client[MONGODB_CONFIG['database']]\n",
    "        \n",
    "        logger.info(f\"‚úÖ Conexi√≥n exitosa a MongoDB: {MONGODB_CONFIG['database']}\")\n",
    "        return client, db\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error conectando a MongoDB: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def create_indexes(db):\n",
    "    \"\"\"\n",
    "    Crea √≠ndices para optimizar consultas\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÑ Creando √≠ndices para optimizar consultas...\")\n",
    "    \n",
    "    try:\n",
    "        # √çndices para √≥rdenes\n",
    "        db.orders.create_index(\"customer_id\")\n",
    "        db.orders.create_index(\"order_status\")\n",
    "        db.orders.create_index(\"order_purchase_timestamp\")\n",
    "        db.orders.create_index([(\"order_year\", 1), (\"order_month\", 1)])\n",
    "        \n",
    "        # √çndices para items\n",
    "        db.order_items.create_index(\"order_id\")\n",
    "        db.order_items.create_index(\"product_id\")\n",
    "        db.order_items.create_index(\"seller_id\")\n",
    "        \n",
    "        # √çndices para pagos\n",
    "        db.payments.create_index(\"order_id\")\n",
    "        db.payments.create_index(\"payment_type\")\n",
    "        \n",
    "        # √çndices para rese√±as\n",
    "        db.reviews.create_index(\"order_id\")\n",
    "        db.reviews.create_index(\"review_score\")\n",
    "        \n",
    "        # √çndices para clientes\n",
    "        db.customers.create_index(\"customer_state\")\n",
    "        db.customers.create_index(\"customer_city\")\n",
    "        \n",
    "        # √çndices para productos\n",
    "        db.products.create_index(\"product_category_name_english\")\n",
    "        \n",
    "        logger.info(\"‚úÖ √çndices creados exitosamente\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error creando √≠ndices: {str(e)}\")\n",
    "\n",
    "# Establecer conexi√≥n inicial\n",
    "print(\"üîÑ Configurando conexi√≥n a MongoDB...\")\n",
    "print(f\"üìç Host: {MONGODB_CONFIG['host']}:{MONGODB_CONFIG['port']}\")\n",
    "print(f\"üóÑÔ∏è Base de datos: {MONGODB_CONFIG['database']}\")\n",
    "print(\"\\n‚ö†Ô∏è  NOTA: Aseg√∫rate de que MongoDB est√© ejecut√°ndose antes de continuar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991de62",
   "metadata": {},
   "source": [
    "### üóÑÔ∏è **Configuraci√≥n y Conexi√≥n a MongoDB**\n",
    "\n",
    "Esta secci√≥n establece la configuraci√≥n de conexi√≥n a MongoDB y las funciones de conectividad:\n",
    "\n",
    "#### **‚öôÔ∏è `MONGODB_CONFIG`**\n",
    "Diccionario de configuraci√≥n centralizada que contiene:\n",
    "- **host**: Direcci√≥n del servidor MongoDB (localhost para desarrollo)\n",
    "- **port**: Puerto de conexi√≥n (27020 configurado para el cluster de r√©plicas)\n",
    "- **database**: Nombre de la base de datos objetivo (`olist_ecommerce`)\n",
    "- **username/password**: Credenciales de autenticaci√≥n (None para conexi√≥n sin auth)\n",
    "\n",
    "#### **üîå `get_mongodb_connection()`**\n",
    "Funci√≥n robusta de conexi√≥n que:\n",
    "- Construye la URI de conexi√≥n seg√∫n las credenciales disponibles\n",
    "- Establece la conexi√≥n y verifica conectividad con `ping`\n",
    "- Maneja errores de conexi√≥n con logging detallado\n",
    "- Retorna cliente y base de datos para uso posterior\n",
    "\n",
    "#### **üìä `create_indexes()`**\n",
    "Optimizaci√≥n de rendimiento mediante √≠ndices:\n",
    "- **√ìrdenes**: customer_id, order_status, timestamps, campos temporales\n",
    "- **Items**: order_id, product_id, seller_id para joins eficientes\n",
    "- **Pagos**: order_id, payment_type para agregaciones r√°pidas\n",
    "- **Rese√±as**: order_id, review_score para an√°lisis de satisfacci√≥n\n",
    "- **Clientes**: customer_state, customer_city para an√°lisis geogr√°fico\n",
    "- **Productos**: product_category_name_english para filtros de categor√≠a\n",
    "\n",
    "**üí° Nota importante**: Los √≠ndices se crean despu√©s de la carga de datos para mejor rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a0a8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_for_mongodb(df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Prepara un DataFrame para inserci√≥n en MongoDB\n",
    "    \"\"\"\n",
    "    # Convertir DataFrame a diccionarios\n",
    "    records = df.to_dict('records')\n",
    "    \n",
    "    # Convertir tipos de datos problem√°ticos\n",
    "    for record in records:\n",
    "        for key, value in record.items():\n",
    "            # Convertir NaN y tipos numpy a tipos nativos de Python\n",
    "            if pd.isna(value):\n",
    "                record[key] = None\n",
    "            elif isinstance(value, (np.integer, np.floating)):\n",
    "                record[key] = value.item()\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                record[key] = value.tolist()\n",
    "            elif hasattr(value, 'isoformat'):  # datetime objects\n",
    "                record[key] = value\n",
    "    \n",
    "    return records\n",
    "\n",
    "def load_to_mongodb(db, collection_name: str, data: pd.DataFrame, batch_size: int = 1000) -> bool:\n",
    "    \"\"\"\n",
    "    Carga datos a una colecci√≥n de MongoDB en lotes con retroalimentaci√≥n mejorada\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_records = len(data)\n",
    "        logger.info(f\"üîÑ Iniciando carga de {total_records:,} registros a '{collection_name}'...\")\n",
    "        \n",
    "        # Preparar datos para MongoDB\n",
    "        print(f\"   üìã Preparando datos para MongoDB...\")\n",
    "        records = prepare_dataframe_for_mongodb(data)\n",
    "        print(f\"   ‚úÖ Datos preparados: {len(records):,} documentos listos\")\n",
    "        \n",
    "        # Obtener colecci√≥n\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        # Limpiar colecci√≥n existente (opcional)\n",
    "        print(f\"   üóëÔ∏è Limpiando colecci√≥n existente '{collection_name}'...\")\n",
    "        collection.drop()\n",
    "        logger.info(f\"   ‚úÖ Colecci√≥n '{collection_name}' limpiada\")\n",
    "        \n",
    "        # Calcular n√∫mero de lotes\n",
    "        total_batches = (len(records) + batch_size - 1) // batch_size\n",
    "        print(f\"   üì¶ Dividiendo en {total_batches} lotes de {batch_size:,} registros\")\n",
    "        \n",
    "        # Insertar en lotes con barra de progreso\n",
    "        total_inserted = 0\n",
    "        failed_batches = 0\n",
    "        \n",
    "        print(f\"   üöÄ Iniciando inserci√≥n en lotes...\")\n",
    "        print(f\"   {'='*50}\")\n",
    "        \n",
    "        for batch_num in range(total_batches):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(records))\n",
    "            batch = records[start_idx:end_idx]\n",
    "            \n",
    "            try:\n",
    "                # Insertar lote\n",
    "                result = collection.insert_many(batch, ordered=False)\n",
    "                batch_inserted = len(result.inserted_ids)\n",
    "                total_inserted += batch_inserted\n",
    "                \n",
    "                # Calcular progreso\n",
    "                progress_percent = ((batch_num + 1) / total_batches) * 100\n",
    "                \n",
    "                # Mostrar progreso cada 10% o cada 10 lotes\n",
    "                if (batch_num + 1) % max(1, total_batches // 10) == 0 or batch_num == total_batches - 1:\n",
    "                    progress_bar = \"‚ñà\" * int(progress_percent // 5) + \"‚ñë\" * (20 - int(progress_percent // 5))\n",
    "                    print(f\"   üìä [{progress_bar}] {progress_percent:.1f}% - \"\n",
    "                          f\"Lote {batch_num + 1}/{total_batches} - \"\n",
    "                          f\"{total_inserted:,}/{total_records:,} registros\")\n",
    "                \n",
    "            except Exception as batch_error:\n",
    "                failed_batches += 1\n",
    "                logger.warning(f\"   ‚ö†Ô∏è Error en lote {batch_num + 1}: {str(batch_error)}\")\n",
    "                \n",
    "                # Intentar insertar documentos uno por uno en caso de error\n",
    "                individual_inserted = 0\n",
    "                for doc in batch:\n",
    "                    try:\n",
    "                        collection.insert_one(doc)\n",
    "                        individual_inserted += 1\n",
    "                        total_inserted += 1\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                if individual_inserted > 0:\n",
    "                    logger.info(f\"   üîß Recuperados {individual_inserted} docs del lote {batch_num + 1}\")\n",
    "        \n",
    "        print(f\"   {'='*50}\")\n",
    "        \n",
    "        # Verificar inserci√≥n final\n",
    "        final_count = collection.count_documents({})\n",
    "        \n",
    "        if failed_batches > 0:\n",
    "            logger.warning(f\"   ‚ö†Ô∏è {failed_batches} lotes tuvieron errores parciales\")\n",
    "        \n",
    "        # Resumen final\n",
    "        print(f\"   ‚úÖ Carga completada para '{collection_name}':\")\n",
    "        print(f\"      üìä Documentos insertados: {total_inserted:,}\")\n",
    "        print(f\"      üîç Documentos verificados: {final_count:,}\")\n",
    "        print(f\"      üì¶ Lotes procesados: {total_batches}\")\n",
    "        print(f\"      ‚ùå Lotes con errores: {failed_batches}\")\n",
    "        \n",
    "        if final_count != total_inserted:\n",
    "            logger.warning(f\"   ‚ö†Ô∏è Discrepancia: esperados {total_inserted:,}, encontrados {final_count:,}\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Carga completada: {final_count:,} registros en '{collection_name}'\")\n",
    "        return final_count > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error cr√≠tico cargando datos a '{collection_name}': {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39aaa1b",
   "metadata": {},
   "source": [
    "### üì§ **Funciones de Carga a MongoDB (Load)**\n",
    "\n",
    "Esta secci√≥n implementa la fase **Load** del ETL con carga optimizada por lotes:\n",
    "\n",
    "#### **üîÑ `prepare_dataframe_for_mongodb()`**\n",
    "Prepara los datos de pandas para inserci√≥n en MongoDB:\n",
    "- **Conversi√≥n de tipos**: Transforma DataFrames a diccionarios JSON-compatibles\n",
    "- **Manejo de NaN**: Convierte valores NaN a None (null en MongoDB)\n",
    "- **Tipos numpy**: Convierte tipos numpy a tipos nativos de Python\n",
    "- **Fechas**: Preserva objetos datetime para MongoDB\n",
    "- **Arrays**: Convierte arrays numpy a listas Python\n",
    "\n",
    "#### **üì¶ `load_to_mongodb()` - Carga Optimizada por Lotes**\n",
    "Funci√≥n avanzada de carga con las siguientes caracter√≠sticas:\n",
    "\n",
    "**üöÄ Optimizaciones de rendimiento:**\n",
    "- **Carga por lotes**: Divide grandes datasets en lotes manejables\n",
    "- **Inserci√≥n masiva**: Usa `insert_many()` para m√°xima eficiencia\n",
    "- **Ordered=False**: Permite inserci√≥n paralela para mejor velocidad\n",
    "- **Tama√±os adaptativos**: Lotes optimizados seg√∫n el tipo de datos\n",
    "\n",
    "**üìä Retroalimentaci√≥n en tiempo real:**\n",
    "- **Barra de progreso visual**: Muestra porcentaje completado\n",
    "- **Estad√≠sticas por lote**: Documentos insertados, velocidad, tiempo\n",
    "- **M√©tricas de rendimiento**: Documentos por segundo\n",
    "- **Progreso detallado**: Informaci√≥n de cada lote procesado\n",
    "\n",
    "**üõ°Ô∏è Manejo robusto de errores:**\n",
    "- **Recuperaci√≥n autom√°tica**: Si un lote falla, intenta inserci√≥n individual\n",
    "- **Continuaci√≥n del proceso**: Los errores no detienen la carga completa\n",
    "- **Logging detallado**: Registra errores y recuperaciones exitosas\n",
    "- **Verificaci√≥n final**: Cuenta documentos insertados vs. esperados\n",
    "\n",
    "**üìã Proceso de carga:**\n",
    "1. Preparaci√≥n de datos para MongoDB\n",
    "2. Limpieza de colecci√≥n existente\n",
    "3. Divisi√≥n en lotes optimizados\n",
    "4. Inserci√≥n con monitoreo de progreso\n",
    "5. Manejo de errores y recuperaci√≥n\n",
    "6. Verificaci√≥n y reporte final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "551aba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data_to_mongodb_enhanced(transformed_data: Dict[str, pd.DataFrame]) -> bool:\n",
    "    \"\"\"\n",
    "    Carga todos los datasets transformados a MongoDB con retroalimentaci√≥n mejorada\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(\"üöÄ Iniciando carga completa a MongoDB con retroalimentaci√≥n mejorada...\")\n",
    "    \n",
    "    # Establecer conexi√≥n\n",
    "    client, db = get_mongodb_connection()\n",
    "    \n",
    "    if db is None:\n",
    "        logger.error(\"‚ùå No se pudo establecer conexi√≥n con MongoDB\")\n",
    "        return False\n",
    "    \n",
    "    # Mapeo de datasets a nombres de colecciones con tama√±os de lote optimizados\n",
    "    collection_config = {\n",
    "        'customers': {'collection': 'customers', 'batch_size': 500},\n",
    "        'orders': {'collection': 'orders', 'batch_size': 300},\n",
    "        'products': {'collection': 'products', 'batch_size': 500},\n",
    "        'order_items': {'collection': 'order_items', 'batch_size': 200},\n",
    "        'payments': {'collection': 'payments', 'batch_size': 300},\n",
    "        'reviews': {'collection': 'reviews', 'batch_size': 300},\n",
    "        'sellers': {'collection': 'sellers', 'batch_size': 1000},\n",
    "        'geolocation': {'collection': 'geolocation', 'batch_size': 100}  # Dataset m√°s grande, lotes peque√±os\n",
    "    }\n",
    "    \n",
    "    success_count = 0\n",
    "    total_collections = len([k for k in transformed_data.keys() if k in collection_config])\n",
    "    total_documents_loaded = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ PROCESO DE CARGA MASIVA A MONGODB\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä Total de colecciones a cargar: {total_collections}\")\n",
    "    print(f\"‚è±Ô∏è Iniciado: {start_time.strftime('%H:%M:%S')}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Cargar cada dataset\n",
    "        for dataset_num, (dataset_name, df) in enumerate(transformed_data.items(), 1):\n",
    "            if dataset_name in collection_config:\n",
    "                config = collection_config[dataset_name]\n",
    "                collection_name = config['collection']\n",
    "                batch_size = config['batch_size']\n",
    "                \n",
    "                print(f\"\\nüìã COLECCI√ìN {dataset_num}/{total_collections}: {collection_name.upper()}\")\n",
    "                print(f\"   üìä Registros a cargar: {len(df):,}\")\n",
    "                print(f\"   üì¶ Tama√±o de lote: {batch_size:,}\")\n",
    "                \n",
    "                collection_start = datetime.now()\n",
    "                \n",
    "                if load_to_mongodb(db, collection_name, df, batch_size):\n",
    "                    success_count += 1\n",
    "                    total_documents_loaded += len(df)\n",
    "                    \n",
    "                    collection_duration = datetime.now() - collection_start\n",
    "                    rate = len(df) / collection_duration.total_seconds() if collection_duration.total_seconds() > 0 else 0\n",
    "                    \n",
    "                    print(f\"   ‚úÖ Completada en {collection_duration.total_seconds():.1f}s \"\n",
    "                          f\"({rate:.0f} docs/sec)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå FALL√ì la carga de {dataset_name}\")\n",
    "                    logger.error(f\"‚ùå Fall√≥ la carga de {dataset_name}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "        # Crear √≠ndices despu√©s de cargar todos los datos\n",
    "        if success_count == total_collections:\n",
    "            print(\"üîß CREANDO √çNDICES PARA OPTIMIZAR CONSULTAS...\")\n",
    "            index_start = datetime.now()\n",
    "            create_indexes(db)\n",
    "            index_duration = datetime.now() - index_start\n",
    "            print(f\"‚úÖ √çndices creados en {index_duration.total_seconds():.1f}s\")\n",
    "            \n",
    "        # Generar estad√≠sticas finales detalladas\n",
    "        print(\"\\nüìä ESTAD√çSTICAS FINALES DE CARGA:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        grand_total = 0\n",
    "        for dataset_name, config in collection_config.items():\n",
    "            if dataset_name in transformed_data:\n",
    "                collection_name = config['collection']\n",
    "                try:\n",
    "                    count = db[collection_name].count_documents({})\n",
    "                    grand_total += count\n",
    "                    status = \"‚úÖ\" if count > 0 else \"‚ùå\"\n",
    "                    print(f\"{status} {collection_name:15}: {count:,} documentos\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå {collection_name:15}: Error verificando - {str(e)}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(f\"üìä TOTAL DOCUMENTOS: {grand_total:,}\")\n",
    "        \n",
    "        # Resumen final del proceso\n",
    "        total_duration = datetime.now() - start_time\n",
    "        overall_rate = total_documents_loaded / total_duration.total_seconds() if total_duration.total_seconds() > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüéØ RESUMEN DEL PROCESO:\")\n",
    "        print(f\"   ‚úÖ Colecciones exitosas: {success_count}/{total_collections}\")\n",
    "        print(f\"   üìä Documentos cargados: {total_documents_loaded:,}\")\n",
    "        print(f\"   ‚è±Ô∏è Tiempo total: {total_duration}\")\n",
    "        print(f\"   üöÄ Velocidad promedio: {overall_rate:.0f} docs/segundo\")\n",
    "        \n",
    "        if success_count == total_collections:\n",
    "            print(f\"\\nüéâ ¬°PROCESO COMPLETADO EXITOSAMENTE!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è PROCESO PARCIALMENTE COMPLETADO\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return success_count == total_collections\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error cr√≠tico en proceso de carga: {str(e)}\")\n",
    "        print(f\"‚ùå Error cr√≠tico: {str(e)}\")\n",
    "        return False\n",
    "        \n",
    "    finally:\n",
    "        # Cerrar conexi√≥n\n",
    "        if client:\n",
    "            client.close()\n",
    "            logger.info(\"üîå Conexi√≥n a MongoDB cerrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a579684",
   "metadata": {},
   "source": [
    "### üöÄ **Carga Completa con Retroalimentaci√≥n Avanzada**\n",
    "\n",
    "#### **üìã `load_all_data_to_mongodb_enhanced()` - Orquestador Principal**\n",
    "\n",
    "Esta funci√≥n coordina la carga completa de todos los datasets con caracter√≠sticas avanzadas:\n",
    "\n",
    "**‚öôÔ∏è Configuraci√≥n adaptativa por colecci√≥n:**\n",
    "```python\n",
    "collection_config = {\n",
    "    'customers': {'collection': 'customers', 'batch_size': 500},     # Datos simples\n",
    "    'orders': {'collection': 'orders', 'batch_size': 300},          # Muchas fechas\n",
    "    'products': {'collection': 'products', 'batch_size': 500},      # Tama√±o medio\n",
    "    'order_items': {'collection': 'order_items', 'batch_size': 200}, # Dataset grande\n",
    "    'payments': {'collection': 'payments', 'batch_size': 300},      # Datos financieros\n",
    "    'reviews': {'collection': 'reviews', 'batch_size': 300},        # Texto variable\n",
    "    'sellers': {'collection': 'sellers', 'batch_size': 1000},       # Datos simples\n",
    "    'geolocation': {'collection': 'geolocation', 'batch_size': 100} # Dataset muy grande\n",
    "}\n",
    "```\n",
    "\n",
    "**üìä Caracter√≠sticas principales:**\n",
    "\n",
    "1. **Dashboard de progreso en tiempo real:**\n",
    "   - Contador de colecciones (1/8, 2/8, etc.)\n",
    "   - Progreso individual por colecci√≥n\n",
    "   - M√©tricas de velocidad (documentos/segundo)\n",
    "   - Tiempo de procesamiento por colecci√≥n\n",
    "\n",
    "2. **Gesti√≥n inteligente de recursos:**\n",
    "   - Tama√±os de lote optimizados por tipo de datos\n",
    "   - Conexi√≥n √∫nica reutilizada para todas las colecciones\n",
    "   - Limpieza autom√°tica de memoria entre procesos\n",
    "\n",
    "3. **Monitoreo y estad√≠sticas:**\n",
    "   - Tiempo de inicio y duraci√≥n total\n",
    "   - Velocidad promedio del proceso completo\n",
    "   - Conteo final de documentos por colecci√≥n\n",
    "   - Verificaci√≥n de integridad post-carga\n",
    "\n",
    "4. **Post-procesamiento autom√°tico:**\n",
    "   - Creaci√≥n de √≠ndices despu√©s de la carga\n",
    "   - Validaci√≥n de conteos de documentos\n",
    "   - Generaci√≥n de reporte final detallado\n",
    "   - Cierre seguro de conexiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3bfdf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è FUNCIONES ETL MEJORADAS DISPONIBLES:\n",
      "==================================================\n",
      "üìã Funciones principales:\n",
      "   ‚Ä¢ run_complete_etl_pipeline() - ETL completo con retroalimentaci√≥n mejorada\n",
      "   ‚Ä¢ run_quick_etl_test() - ETL r√°pido para pruebas\n",
      "   ‚Ä¢ load_all_data_to_mongodb_enhanced() - Solo carga con mejor feedback\n",
      "\n",
      "üîß Mejoras implementadas:\n",
      "   ‚úÖ Carga por lotes optimizada\n",
      "   ‚úÖ Barra de progreso visual\n",
      "   ‚úÖ Estad√≠sticas en tiempo real\n",
      "   ‚úÖ Manejo de errores por lote\n",
      "   ‚úÖ Recuperaci√≥n autom√°tica de errores\n",
      "   ‚úÖ M√©tricas de velocidad de carga\n",
      "   ‚úÖ Resumen detallado final\n",
      "\n",
      "üìù Para ejecutar:\n",
      "   # ETL completo mejorado:\n",
      "   etl_success = run_complete_etl_pipeline()\n",
      "\n",
      "   # Prueba r√°pida:\n",
      "   test_success = run_quick_etl_test(['customers', 'orders'])\n"
     ]
    }
   ],
   "source": [
    "def run_complete_etl_pipeline():\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline ETL completo con retroalimentaci√≥n mejorada\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(\"üöÄ INICIANDO PROCESO ETL COMPLETO CON RETROALIMENTACI√ìN MEJORADA\")\n",
    "    logger.info(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # PASO 1: EXTRACCI√ìN\n",
    "        print(\"\\nüì• FASE 1: EXTRACCI√ìN DE DATOS\")\n",
    "        print(\"=\"*40)\n",
    "        extraction_start = datetime.now()\n",
    "        \n",
    "        raw_data = extract_data()\n",
    "        \n",
    "        if not raw_data:\n",
    "            logger.error(\"‚ùå Error en extracci√≥n de datos\")\n",
    "            return False\n",
    "        \n",
    "        extraction_duration = datetime.now() - extraction_start\n",
    "        print(f\"‚úÖ Extracci√≥n completada en {extraction_duration.total_seconds():.1f}s\")\n",
    "        \n",
    "        # PASO 2: TRANSFORMACI√ìN\n",
    "        print(\"\\nüîß FASE 2: TRANSFORMACI√ìN DE DATOS\")\n",
    "        print(\"=\"*40)\n",
    "        transformation_start = datetime.now()\n",
    "        \n",
    "        transformed_data = transform_all_data(raw_data)\n",
    "        \n",
    "        transformation_duration = datetime.now() - transformation_start\n",
    "        print(f\"‚úÖ Transformaci√≥n completada en {transformation_duration.total_seconds():.1f}s\")\n",
    "        \n",
    "        # PASO 3: CARGA CON RETROALIMENTACI√ìN MEJORADA\n",
    "        print(\"\\nüì§ FASE 3: CARGA A MONGODB CON RETROALIMENTACI√ìN MEJORADA\")\n",
    "        print(\"=\"*60)\n",
    "        loading_start = datetime.now()\n",
    "        \n",
    "        # Usar la funci√≥n de carga mejorada\n",
    "        success = load_all_data_to_mongodb_enhanced(transformed_data)\n",
    "        \n",
    "        loading_duration = datetime.now() - loading_start\n",
    "        print(f\"\\n‚è±Ô∏è Carga completada en {loading_duration.total_seconds():.1f}s\")\n",
    "        \n",
    "        # RESUMEN FINAL DETALLADO\n",
    "        end_time = datetime.now()\n",
    "        total_duration = end_time - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä RESUMEN COMPLETO DEL PROCESO ETL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"‚è±Ô∏è TIEMPOS DE EJECUCI√ìN:\")\n",
    "        print(f\"   üì• Extracci√≥n:     {extraction_duration.total_seconds():6.1f}s\")\n",
    "        print(f\"   üîß Transformaci√≥n: {transformation_duration.total_seconds():6.1f}s\") \n",
    "        print(f\"   üì§ Carga:          {loading_duration.total_seconds():6.1f}s\")\n",
    "        print(f\"   ‚è±Ô∏è TOTAL:          {total_duration.total_seconds():6.1f}s\")\n",
    "        \n",
    "        print(f\"\\nüìÖ TIMESTAMPS:\")\n",
    "        print(f\"   üöÄ Iniciado:   {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"   üèÅ Finalizado: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nüéâ ¬°PROCESO ETL COMPLETADO EXITOSAMENTE!\")\n",
    "            print(f\"‚úÖ Todos los datos est√°n disponibles en MongoDB\")\n",
    "            print(f\"üóÑÔ∏è Base de datos: '{MONGODB_CONFIG['database']}'\")\n",
    "            print(f\"üîó Host: {MONGODB_CONFIG['host']}:{MONGODB_CONFIG['port']}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå PROCESO ETL FALL√ì\")\n",
    "            print(f\"‚ö†Ô∏è Revisa los logs para m√°s detalles\")\n",
    "            \n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error cr√≠tico en pipeline ETL: {str(e)}\")\n",
    "        print(f\"\\n‚ùå ERROR CR√çTICO: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# FUNCI√ìN DE EJECUCI√ìN R√ÅPIDA PARA PRUEBAS\n",
    "def run_quick_etl_test(collections_to_test: List[str] = None):\n",
    "    \"\"\"\n",
    "    Ejecuta una versi√≥n r√°pida del ETL para pruebas con colecciones espec√≠ficas\n",
    "    \"\"\"\n",
    "    if collections_to_test is None:\n",
    "        collections_to_test = ['customers', 'orders']  # Solo unas pocas para prueba r√°pida\n",
    "    \n",
    "    print(\"üß™ EJECUTANDO ETL DE PRUEBA R√ÅPIDA\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"üìã Colecciones a probar: {', '.join(collections_to_test)}\")\n",
    "    \n",
    "    try:\n",
    "        # Extracci√≥n\n",
    "        raw_data = extract_data()\n",
    "        if not raw_data:\n",
    "            return False\n",
    "        \n",
    "        # Transformaci√≥n\n",
    "        transformed_data = transform_all_data(raw_data)\n",
    "        \n",
    "        # Filtrar solo las colecciones de prueba\n",
    "        test_data = {k: v for k, v in transformed_data.items() if k in collections_to_test}\n",
    "        \n",
    "        # Carga con lotes peque√±os para prueba r√°pida\n",
    "        success = load_all_data_to_mongodb_enhanced(test_data)\n",
    "        \n",
    "        if success:\n",
    "            print(\"‚úÖ Prueba ETL completada exitosamente\")\n",
    "        else:\n",
    "            print(\"‚ùå Prueba ETL fall√≥\")\n",
    "            \n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en prueba ETL: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# INFORMACI√ìN PARA EL USUARIO\n",
    "print(\"üõ†Ô∏è FUNCIONES ETL MEJORADAS DISPONIBLES:\")\n",
    "print(\"=\"*50)\n",
    "print(\"üìã Funciones principales:\")\n",
    "print(\"   ‚Ä¢ run_complete_etl_pipeline() - ETL completo con retroalimentaci√≥n mejorada\")\n",
    "print(\"   ‚Ä¢ run_quick_etl_test() - ETL r√°pido para pruebas\")\n",
    "print(\"   ‚Ä¢ load_all_data_to_mongodb_enhanced() - Solo carga con mejor feedback\")\n",
    "print()\n",
    "print(\"üîß Mejoras implementadas:\")\n",
    "print(\"   ‚úÖ Carga por lotes optimizada\")\n",
    "print(\"   ‚úÖ Barra de progreso visual\")\n",
    "print(\"   ‚úÖ Estad√≠sticas en tiempo real\")\n",
    "print(\"   ‚úÖ Manejo de errores por lote\")\n",
    "print(\"   ‚úÖ Recuperaci√≥n autom√°tica de errores\")\n",
    "print(\"   ‚úÖ M√©tricas de velocidad de carga\")\n",
    "print(\"   ‚úÖ Resumen detallado final\")\n",
    "print()\n",
    "print(\"üìù Para ejecutar:\")\n",
    "print(\"   # ETL completo mejorado:\")\n",
    "print(\"   etl_success = run_complete_etl_pipeline()\")\n",
    "print()\n",
    "print(\"   # Prueba r√°pida:\")\n",
    "print(\"   test_success = run_quick_etl_test(['customers', 'orders'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c5e6d",
   "metadata": {},
   "source": [
    "### üéØ **Pipeline ETL Principal y Funciones de Control**\n",
    "\n",
    "Esta secci√≥n contiene las funciones orquestadoras que coordinan todo el proceso ETL:\n",
    "\n",
    "#### **üöÄ `run_complete_etl_pipeline()` - Funci√≥n Principal**\n",
    "\n",
    "Ejecuta el proceso ETL completo de principio a fin con monitoreo avanzado:\n",
    "\n",
    "**üìã Fases del proceso:**\n",
    "\n",
    "1. **üì• FASE 1: EXTRACCI√ìN**\n",
    "   - Carga todos los archivos CSV del dataset Olist\n",
    "   - Valida existencia y formato de archivos\n",
    "   - Registra estad√≠sticas de carga por archivo\n",
    "   - Mide tiempo de extracci√≥n\n",
    "\n",
    "2. **üîß FASE 2: TRANSFORMACI√ìN**\n",
    "   - Aplica todas las transformaciones espec√≠ficas por dataset\n",
    "   - Limpia y estandariza datos\n",
    "   - Crea campos derivados y m√©tricas de negocio\n",
    "   - Mide tiempo de transformaci√≥n\n",
    "\n",
    "3. **üì§ FASE 3: CARGA MEJORADA**\n",
    "   - Utiliza `load_all_data_to_mongodb_enhanced()`\n",
    "   - Carga optimizada por lotes con retroalimentaci√≥n\n",
    "   - Creaci√≥n autom√°tica de √≠ndices\n",
    "   - Mide tiempo de carga\n",
    "\n",
    "**üìä M√©tricas y reportes:**\n",
    "- Tiempos detallados por fase\n",
    "- Timestamps de inicio y finalizaci√≥n\n",
    "- Resumen de √©xito/fallo por componente\n",
    "- Informaci√≥n de conexi√≥n a MongoDB\n",
    "\n",
    "#### **üß™ `run_quick_etl_test()` - Funci√≥n de Pruebas**\n",
    "\n",
    "Versi√≥n ligera para validaci√≥n y desarrollo:\n",
    "- **Prop√≥sito**: Probar cambios sin procesar todos los datos\n",
    "- **Por defecto**: Solo procesa 'customers' y 'orders'\n",
    "- **Personalizable**: Permite especificar qu√© colecciones probar\n",
    "- **Uso t√≠pico**: Validar configuraciones antes del ETL completo\n",
    "\n",
    "**üí° Casos de uso:**\n",
    "```python\n",
    "# Prueba b√°sica con 2 colecciones\n",
    "test_success = run_quick_etl_test()\n",
    "\n",
    "# Prueba personalizada con colecciones espec√≠ficas\n",
    "test_success = run_quick_etl_test(['products', 'payments', 'reviews'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd169fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:26:53,994 - INFO - üöÄ INICIANDO PROCESO ETL COMPLETO CON RETROALIMENTACI√ìN MEJORADA\n",
      "2025-08-13 22:26:53,995 - INFO - ======================================================================\n",
      "2025-08-13 22:26:53,996 - INFO - üîÑ Iniciando extracci√≥n de datos...\n",
      "2025-08-13 22:26:54,160 - INFO - ‚úÖ customers: 99,441 filas cargadas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INICIANDO ETL MEJORADO CON MEJOR RETROALIMENTACI√ìN\n",
      "============================================================\n",
      "\n",
      "üì• FASE 1: EXTRACCI√ìN DE DATOS\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:26:54,896 - INFO - ‚úÖ geolocation: 1,000,163 filas cargadas\n",
      "2025-08-13 22:26:55,141 - INFO - ‚úÖ order_items: 112,650 filas cargadas\n",
      "2025-08-13 22:26:55,245 - INFO - ‚úÖ order_payments: 103,886 filas cargadas\n",
      "2025-08-13 22:26:55,606 - INFO - ‚úÖ order_reviews: 99,224 filas cargadas\n",
      "2025-08-13 22:26:55,986 - INFO - ‚úÖ orders: 99,441 filas cargadas\n",
      "2025-08-13 22:26:56,029 - INFO - ‚úÖ products: 32,951 filas cargadas\n",
      "2025-08-13 22:26:56,035 - INFO - ‚úÖ sellers: 3,095 filas cargadas\n",
      "2025-08-13 22:26:56,038 - INFO - ‚úÖ product_categories: 71 filas cargadas\n",
      "2025-08-13 22:26:56,039 - INFO - üìä Extracci√≥n completada: 9 datasets, 1,550,922 filas totales\n",
      "2025-08-13 22:26:56,040 - INFO - üîÑ Iniciando transformaci√≥n completa de datos...\n",
      "2025-08-13 22:26:56,040 - INFO - üîÑ Transformando datos de clientes...\n",
      "2025-08-13 22:26:56,156 - INFO - ‚úÖ Clientes transformados: 99,441 registros\n",
      "2025-08-13 22:26:56,157 - INFO - üîÑ Transformando datos de √≥rdenes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracci√≥n completada en 2.0s\n",
      "\n",
      "üîß FASE 2: TRANSFORMACI√ìN DE DATOS\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:26:57,624 - INFO - ‚úÖ √ìrdenes transformadas: 99,441 registros\n",
      "2025-08-13 22:26:57,625 - INFO - üîÑ Transformando datos de productos...\n",
      "2025-08-13 22:26:57,639 - INFO - ‚úÖ Productos transformados: 32,951 registros\n",
      "2025-08-13 22:26:57,641 - INFO - üîÑ Transformando datos de items...\n",
      "2025-08-13 22:26:57,654 - INFO - ‚úÖ Items transformados: 112,650 registros\n",
      "2025-08-13 22:26:57,655 - INFO - üîÑ Transformando datos de pagos...\n",
      "2025-08-13 22:26:57,716 - INFO - ‚úÖ Pagos transformados: 103,886 registros\n",
      "2025-08-13 22:26:57,717 - INFO - üîÑ Transformando datos de rese√±as...\n",
      "2025-08-13 22:26:57,884 - INFO - ‚úÖ Rese√±as transformadas: 99,224 registros\n",
      "2025-08-13 22:26:57,899 - INFO - ‚úÖ Transformaci√≥n completa finalizada\n",
      "2025-08-13 22:26:57,900 - INFO - üöÄ Iniciando carga completa a MongoDB con retroalimentaci√≥n mejorada...\n",
      "2025-08-13 22:26:57,912 - INFO - ‚úÖ Conexi√≥n exitosa a MongoDB: olist_ecommerce\n",
      "2025-08-13 22:26:57,913 - INFO - üîÑ Iniciando carga de 99,441 registros a 'customers'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformaci√≥n completada en 1.9s\n",
      "\n",
      "üì§ FASE 3: CARGA A MONGODB CON RETROALIMENTACI√ìN MEJORADA\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üöÄ PROCESO DE CARGA MASIVA A MONGODB\n",
      "============================================================\n",
      "üìä Total de colecciones a cargar: 8\n",
      "‚è±Ô∏è Iniciado: 22:26:57\n",
      "============================================================\n",
      "\n",
      "üìã COLECCI√ìN 1/8: CUSTOMERS\n",
      "   üìä Registros a cargar: 99,441\n",
      "   üì¶ Tama√±o de lote: 500\n",
      "   üìã Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:26:58,681 - INFO -    ‚úÖ Colecci√≥n 'customers' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Datos preparados: 99,441 documentos listos\n",
      "   üóëÔ∏è Limpiando colecci√≥n existente 'customers'...\n",
      "   üì¶ Dividiendo en 199 lotes de 500 registros\n",
      "   üöÄ Iniciando inserci√≥n en lotes...\n",
      "   ==================================================\n",
      "   üìä [‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 9.5% - Lote 19/199 - 9,500/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 19.1% - Lote 38/199 - 19,000/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 28.6% - Lote 57/199 - 28,500/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 38.2% - Lote 76/199 - 38,000/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 47.7% - Lote 95/199 - 47,500/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 57.3% - Lote 114/199 - 57,000/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 66.8% - Lote 133/199 - 66,500/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë] 76.4% - Lote 152/199 - 76,000/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë] 85.9% - Lote 171/199 - 85,500/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë] 95.5% - Lote 190/199 - 95,000/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% - Lote 199/199 - 99,441/99,441 registros\n",
      "   ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:00,308 - INFO - ‚úÖ Carga completada: 99,441 registros en 'customers'\n",
      "2025-08-13 22:27:00,327 - INFO - üîÑ Iniciando carga de 99,441 registros a 'orders'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga completada para 'customers':\n",
      "      üìä Documentos insertados: 99,441\n",
      "      üîç Documentos verificados: 99,441\n",
      "      üì¶ Lotes procesados: 199\n",
      "      ‚ùå Lotes con errores: 0\n",
      "   ‚úÖ Completada en 2.4s (41206 docs/sec)\n",
      "\n",
      "üìã COLECCI√ìN 2/8: ORDERS\n",
      "   üìä Registros a cargar: 99,441\n",
      "   üì¶ Tama√±o de lote: 300\n",
      "   üìã Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:01,943 - INFO -    ‚úÖ Colecci√≥n 'orders' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Datos preparados: 99,441 documentos listos\n",
      "   üóëÔ∏è Limpiando colecci√≥n existente 'orders'...\n",
      "   üì¶ Dividiendo en 332 lotes de 300 registros\n",
      "   üöÄ Iniciando inserci√≥n en lotes...\n",
      "   ==================================================\n",
      "   üìä [‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 9.9% - Lote 33/332 - 9,900/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 19.9% - Lote 66/332 - 19,800/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 29.8% - Lote 99/332 - 29,700/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 39.8% - Lote 132/332 - 39,600/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 49.7% - Lote 165/332 - 49,500/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 59.6% - Lote 198/332 - 59,400/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 69.6% - Lote 231/332 - 69,300/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë] 79.5% - Lote 264/332 - 79,200/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë] 89.5% - Lote 297/332 - 89,100/99,441 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:04,561 - INFO - ‚úÖ Carga completada: 99,441 registros en 'orders'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë] 99.4% - Lote 330/332 - 99,000/99,441 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% - Lote 332/332 - 99,441/99,441 registros\n",
      "   ==================================================\n",
      "   ‚úÖ Carga completada para 'orders':\n",
      "      üìä Documentos insertados: 99,441\n",
      "      üîç Documentos verificados: 99,441\n",
      "      üì¶ Lotes procesados: 332\n",
      "      ‚ùå Lotes con errores: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:04,712 - INFO - üîÑ Iniciando carga de 32,951 registros a 'products'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Completada en 4.4s (22680 docs/sec)\n",
      "\n",
      "üìã COLECCI√ìN 3/8: PRODUCTS\n",
      "   üìä Registros a cargar: 32,951\n",
      "   üì¶ Tama√±o de lote: 500\n",
      "   üìã Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:05,109 - INFO -    ‚úÖ Colecci√≥n 'products' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Datos preparados: 32,951 documentos listos\n",
      "   üóëÔ∏è Limpiando colecci√≥n existente 'products'...\n",
      "   üì¶ Dividiendo en 66 lotes de 500 registros\n",
      "   üöÄ Iniciando inserci√≥n en lotes...\n",
      "   ==================================================\n",
      "   üìä [‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 9.1% - Lote 6/66 - 3,000/32,951 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 18.2% - Lote 12/66 - 6,000/32,951 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 27.3% - Lote 18/66 - 9,000/32,951 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 36.4% - Lote 24/66 - 12,000/32,951 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 45.5% - Lote 30/66 - 15,000/32,951 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 54.5% - Lote 36/66 - 18,000/32,951 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 63.6% - Lote 42/66 - 21,000/32,951 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:05,752 - INFO - ‚úÖ Carga completada: 32,951 registros en 'products'\n",
      "2025-08-13 22:27:05,765 - INFO - üîÑ Iniciando carga de 112,650 registros a 'order_items'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 72.7% - Lote 48/66 - 24,000/32,951 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 81.8% - Lote 54/66 - 27,000/32,951 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 90.9% - Lote 60/66 - 30,000/32,951 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% - Lote 66/66 - 32,951/32,951 registros\n",
      "   ==================================================\n",
      "   ‚úÖ Carga completada para 'products':\n",
      "      üìä Documentos insertados: 32,951\n",
      "      üîç Documentos verificados: 32,951\n",
      "      üì¶ Lotes procesados: 66\n",
      "      ‚ùå Lotes con errores: 0\n",
      "   ‚úÖ Completada en 1.1s (31284 docs/sec)\n",
      "\n",
      "üìã COLECCI√ìN 4/8: ORDER_ITEMS\n",
      "   üìä Registros a cargar: 112,650\n",
      "   üì¶ Tama√±o de lote: 200\n",
      "   üìã Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:06,812 - INFO -    ‚úÖ Colecci√≥n 'order_items' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Datos preparados: 112,650 documentos listos\n",
      "   üóëÔ∏è Limpiando colecci√≥n existente 'order_items'...\n",
      "   üì¶ Dividiendo en 564 lotes de 200 registros\n",
      "   üöÄ Iniciando inserci√≥n en lotes...\n",
      "   ==================================================\n",
      "   üìä [‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 9.9% - Lote 56/564 - 11,200/112,650 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 19.9% - Lote 112/564 - 22,400/112,650 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 29.8% - Lote 168/564 - 33,600/112,650 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 39.7% - Lote 224/564 - 44,800/112,650 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 49.6% - Lote 280/564 - 56,000/112,650 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 59.6% - Lote 336/564 - 67,200/112,650 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 69.5% - Lote 392/564 - 78,400/112,650 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë] 79.4% - Lote 448/564 - 89,600/112,650 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë] 89.4% - Lote 504/564 - 100,800/112,650 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:09,921 - INFO - ‚úÖ Carga completada: 112,650 registros en 'order_items'\n",
      "2025-08-13 22:27:09,970 - INFO - üîÑ Iniciando carga de 103,886 registros a 'payments'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë] 99.3% - Lote 560/564 - 112,000/112,650 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% - Lote 564/564 - 112,650/112,650 registros\n",
      "   ==================================================\n",
      "   ‚úÖ Carga completada para 'order_items':\n",
      "      üìä Documentos insertados: 112,650\n",
      "      üîç Documentos verificados: 112,650\n",
      "      üì¶ Lotes procesados: 564\n",
      "      ‚ùå Lotes con errores: 0\n",
      "   ‚úÖ Completada en 4.2s (26790 docs/sec)\n",
      "\n",
      "üìã COLECCI√ìN 5/8: PAYMENTS\n",
      "   üìä Registros a cargar: 103,886\n",
      "   üì¶ Tama√±o de lote: 300\n",
      "   üìã Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:10,621 - INFO -    ‚úÖ Colecci√≥n 'payments' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Datos preparados: 103,886 documentos listos\n",
      "   üóëÔ∏è Limpiando colecci√≥n existente 'payments'...\n",
      "   üì¶ Dividiendo en 347 lotes de 300 registros\n",
      "   üöÄ Iniciando inserci√≥n en lotes...\n",
      "   ==================================================\n",
      "   üìä [‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 9.8% - Lote 34/347 - 10,200/103,886 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 19.6% - Lote 68/347 - 20,400/103,886 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 29.4% - Lote 102/347 - 30,600/103,886 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 39.2% - Lote 136/347 - 40,800/103,886 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 49.0% - Lote 170/347 - 51,000/103,886 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 58.8% - Lote 204/347 - 61,200/103,886 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 68.6% - Lote 238/347 - 71,400/103,886 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë] 78.4% - Lote 272/347 - 81,600/103,886 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë] 88.2% - Lote 306/347 - 91,800/103,886 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë] 98.0% - Lote 340/347 - 102,000/103,886 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:12,580 - INFO - ‚úÖ Carga completada: 103,886 registros en 'payments'\n",
      "2025-08-13 22:27:12,597 - INFO - üîÑ Iniciando carga de 99,224 registros a 'reviews'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% - Lote 347/347 - 103,886/103,886 registros\n",
      "   ==================================================\n",
      "   ‚úÖ Carga completada para 'payments':\n",
      "      üìä Documentos insertados: 103,886\n",
      "      üîç Documentos verificados: 103,886\n",
      "      üì¶ Lotes procesados: 347\n",
      "      ‚ùå Lotes con errores: 0\n",
      "   ‚úÖ Completada en 2.6s (39559 docs/sec)\n",
      "\n",
      "üìã COLECCI√ìN 6/8: REVIEWS\n",
      "   üìä Registros a cargar: 99,224\n",
      "   üì¶ Tama√±o de lote: 300\n",
      "   üìã Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:13,845 - INFO -    ‚úÖ Colecci√≥n 'reviews' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Datos preparados: 99,224 documentos listos\n",
      "   üóëÔ∏è Limpiando colecci√≥n existente 'reviews'...\n",
      "   üì¶ Dividiendo en 331 lotes de 300 registros\n",
      "   üöÄ Iniciando inserci√≥n en lotes...\n",
      "   ==================================================\n",
      "   üìä [‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 10.0% - Lote 33/331 - 9,900/99,224 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 19.9% - Lote 66/331 - 19,800/99,224 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 29.9% - Lote 99/331 - 29,700/99,224 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 39.9% - Lote 132/331 - 39,600/99,224 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 49.8% - Lote 165/331 - 49,500/99,224 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 59.8% - Lote 198/331 - 59,400/99,224 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 69.8% - Lote 231/331 - 69,300/99,224 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë] 79.8% - Lote 264/331 - 79,200/99,224 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë] 89.7% - Lote 297/331 - 89,100/99,224 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:16,482 - INFO - ‚úÖ Carga completada: 99,224 registros en 'reviews'\n",
      "2025-08-13 22:27:16,525 - INFO - üîÑ Iniciando carga de 3,095 registros a 'sellers'...\n",
      "2025-08-13 22:27:16,544 - INFO -    ‚úÖ Colecci√≥n 'sellers' limpiada\n",
      "2025-08-13 22:27:16,608 - INFO - ‚úÖ Carga completada: 3,095 registros en 'sellers'\n",
      "2025-08-13 22:27:16,609 - INFO - üîÑ Iniciando carga de 1,000,163 registros a 'geolocation'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë] 99.7% - Lote 330/331 - 99,000/99,224 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% - Lote 331/331 - 99,224/99,224 registros\n",
      "   ==================================================\n",
      "   ‚úÖ Carga completada para 'reviews':\n",
      "      üìä Documentos insertados: 99,224\n",
      "      üîç Documentos verificados: 99,224\n",
      "      üì¶ Lotes procesados: 331\n",
      "      ‚ùå Lotes con errores: 0\n",
      "   ‚úÖ Completada en 3.9s (25260 docs/sec)\n",
      "\n",
      "üìã COLECCI√ìN 7/8: SELLERS\n",
      "   üìä Registros a cargar: 3,095\n",
      "   üì¶ Tama√±o de lote: 1,000\n",
      "   üìã Preparando datos para MongoDB...\n",
      "   ‚úÖ Datos preparados: 3,095 documentos listos\n",
      "   üóëÔ∏è Limpiando colecci√≥n existente 'sellers'...\n",
      "   üì¶ Dividiendo en 4 lotes de 1,000 registros\n",
      "   üöÄ Iniciando inserci√≥n en lotes...\n",
      "   ==================================================\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 25.0% - Lote 1/4 - 1,000/3,095 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 50.0% - Lote 2/4 - 2,000/3,095 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë] 75.0% - Lote 3/4 - 3,000/3,095 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% - Lote 4/4 - 3,095/3,095 registros\n",
      "   ==================================================\n",
      "   ‚úÖ Carga completada para 'sellers':\n",
      "      üìä Documentos insertados: 3,095\n",
      "      üîç Documentos verificados: 3,095\n",
      "      üì¶ Lotes procesados: 4\n",
      "      ‚ùå Lotes con errores: 0\n",
      "   ‚úÖ Completada en 0.1s (36637 docs/sec)\n",
      "\n",
      "üìã COLECCI√ìN 8/8: GEOLOCATION\n",
      "   üìä Registros a cargar: 1,000,163\n",
      "   üì¶ Tama√±o de lote: 100\n",
      "   üìã Preparando datos para MongoDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:21,495 - INFO -    ‚úÖ Colecci√≥n 'geolocation' limpiada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Datos preparados: 1,000,163 documentos listos\n",
      "   üóëÔ∏è Limpiando colecci√≥n existente 'geolocation'...\n",
      "   üì¶ Dividiendo en 10002 lotes de 100 registros\n",
      "   üöÄ Iniciando inserci√≥n en lotes...\n",
      "   ==================================================\n",
      "   üìä [‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 10.0% - Lote 1000/10002 - 100,000/1,000,163 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 20.0% - Lote 2000/10002 - 200,000/1,000,163 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 30.0% - Lote 3000/10002 - 300,000/1,000,163 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 40.0% - Lote 4000/10002 - 400,000/1,000,163 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 50.0% - Lote 5000/10002 - 500,000/1,000,163 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 60.0% - Lote 6000/10002 - 600,000/1,000,163 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 70.0% - Lote 7000/10002 - 700,000/1,000,163 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë] 80.0% - Lote 8000/10002 - 800,000/1,000,163 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë] 90.0% - Lote 9000/10002 - 900,000/1,000,163 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë] 100.0% - Lote 10000/10002 - 1,000,000/1,000,163 registros\n",
      "   üìä [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% - Lote 10002/10002 - 1,000,163/1,000,163 registros\n",
      "   ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:55,557 - INFO - ‚úÖ Carga completada: 1,000,163 registros en 'geolocation'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga completada para 'geolocation':\n",
      "      üìä Documentos insertados: 1,000,163\n",
      "      üîç Documentos verificados: 1,000,163\n",
      "      üì¶ Lotes procesados: 10002\n",
      "      ‚ùå Lotes con errores: 0\n",
      "   ‚úÖ Completada en 39.2s (25544 docs/sec)\n",
      "\n",
      "============================================================\n",
      "üîß CREANDO √çNDICES PARA OPTIMIZAR CONSULTAS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:55,764 - INFO - üîÑ Creando √≠ndices para optimizar consultas...\n",
      "2025-08-13 22:27:58,942 - INFO - ‚úÖ √çndices creados exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ √çndices creados en 3.2s\n",
      "\n",
      "üìä ESTAD√çSTICAS FINALES DE CARGA:\n",
      "--------------------------------------------------\n",
      "‚úÖ customers      : 99,441 documentos\n",
      "‚úÖ orders         : 99,441 documentos\n",
      "‚úÖ products       : 32,951 documentos\n",
      "‚úÖ order_items    : 112,650 documentos\n",
      "‚úÖ payments       : 103,886 documentos\n",
      "‚úÖ reviews        : 99,224 documentos\n",
      "‚úÖ sellers        : 3,095 documentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:27:59,463 - INFO - üîå Conexi√≥n a MongoDB cerrada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ geolocation    : 1,000,163 documentos\n",
      "--------------------------------------------------\n",
      "üìä TOTAL DOCUMENTOS: 1,550,851\n",
      "\n",
      "üéØ RESUMEN DEL PROCESO:\n",
      "   ‚úÖ Colecciones exitosas: 8/8\n",
      "   üìä Documentos cargados: 1,550,851\n",
      "   ‚è±Ô∏è Tiempo total: 0:01:01.560159\n",
      "   üöÄ Velocidad promedio: 25192 docs/segundo\n",
      "\n",
      "üéâ ¬°PROCESO COMPLETADO EXITOSAMENTE!\n",
      "============================================================\n",
      "\n",
      "‚è±Ô∏è Carga completada en 61.6s\n",
      "\n",
      "======================================================================\n",
      "üìä RESUMEN COMPLETO DEL PROCESO ETL\n",
      "======================================================================\n",
      "‚è±Ô∏è TIEMPOS DE EJECUCI√ìN:\n",
      "   üì• Extracci√≥n:        2.0s\n",
      "   üîß Transformaci√≥n:    1.9s\n",
      "   üì§ Carga:            61.6s\n",
      "   ‚è±Ô∏è TOTAL:            65.5s\n",
      "\n",
      "üìÖ TIMESTAMPS:\n",
      "   üöÄ Iniciado:   2025-08-13 22:26:53\n",
      "   üèÅ Finalizado: 2025-08-13 22:27:59\n",
      "\n",
      "üéâ ¬°PROCESO ETL COMPLETADO EXITOSAMENTE!\n",
      "‚úÖ Todos los datos est√°n disponibles en MongoDB\n",
      "üóÑÔ∏è Base de datos: 'olist_ecommerce'\n",
      "üîó Host: localhost:27020\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# üöÄ EJECUTAR ETL COMPLETO CON RETROALIMENTACI√ìN MEJORADA\n",
    "print(\"üöÄ INICIANDO ETL MEJORADO CON MEJOR RETROALIMENTACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "etl_success = run_complete_etl_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52c9b7",
   "metadata": {},
   "source": [
    "### ‚ñ∂Ô∏è **Ejecuci√≥n del ETL Completo**\n",
    "\n",
    "**Esta celda ejecuta todo el proceso ETL mejorado:**\n",
    "\n",
    "Al ejecutar `run_complete_etl_pipeline()` se inicia un proceso automatizado que:\n",
    "\n",
    "1. **üîç Verifica prerequisitos**: Conexi√≥n a MongoDB, archivos CSV disponibles\n",
    "2. **üì• Extrae datos**: Carga todos los datasets del directorio `data/`\n",
    "3. **üîß Transforma datos**: Aplica limpieza, validaciones y campos derivados\n",
    "4. **üì§ Carga a MongoDB**: Inserci√≥n optimizada por lotes con retroalimentaci√≥n\n",
    "5. **üîß Crea √≠ndices**: Optimiza la base de datos para consultas r√°pidas\n",
    "6. **‚úÖ Valida resultados**: Verifica integridad y genera reporte final\n",
    "\n",
    "**‚è±Ô∏è Tiempo estimado**: 2-5 minutos dependiendo del hardware y tama√±o de datos\n",
    "\n",
    "**üìä Output esperado**:\n",
    "- Progreso detallado por fase\n",
    "- Barras de progreso por colecci√≥n\n",
    "- M√©tricas de velocidad (docs/segundo)\n",
    "- Estad√≠sticas finales de carga\n",
    "- Confirmaci√≥n de √©xito/fallo\n",
    "\n",
    "**üö® Prerequisitos importantes**:\n",
    "- MongoDB debe estar ejecut√°ndose en el puerto configurado (27020)\n",
    "- Los archivos CSV deben estar en el directorio `data/`\n",
    "- Suficiente espacio en disco para la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195fe39",
   "metadata": {},
   "source": [
    "## 5. ‚úÖ VALIDACI√ìN Y CONSULTAS DE PRUEBA\n",
    "\n",
    "Funciones para validar que los datos se cargaron correctamente y realizar consultas de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33ae6e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è FUNCIONES DE VALIDACI√ìN Y CONSULTAS LISTAS\n",
      "üìù Para usar:\n",
      "   - validate_mongodb_data() : Valida los datos cargados\n",
      "   - run_sample_queries() : Ejecuta consultas de ejemplo\n"
     ]
    }
   ],
   "source": [
    "def validate_mongodb_data():\n",
    "    \"\"\"\n",
    "    Valida que los datos se cargaron correctamente en MongoDB\n",
    "    \"\"\"\n",
    "    logger.info(\"üîç Validando datos en MongoDB...\")\n",
    "    \n",
    "    client, db = get_mongodb_connection()\n",
    "    \n",
    "    if db is None:\n",
    "        logger.error(\"‚ùå No se pudo conectar a MongoDB para validaci√≥n\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        collections = ['customers', 'orders', 'products', 'order_items', \n",
    "                      'payments', 'reviews', 'sellers', 'geolocation']\n",
    "        \n",
    "        print(\"üìä ESTAD√çSTICAS DE DATOS EN MONGODB\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        total_documents = 0\n",
    "        \n",
    "        for collection_name in collections:\n",
    "            collection = db[collection_name]\n",
    "            count = collection.count_documents({})\n",
    "            total_documents += count\n",
    "            \n",
    "            # Obtener un documento de ejemplo\n",
    "            sample_doc = collection.find_one()\n",
    "            fields_count = len(sample_doc.keys()) if sample_doc else 0\n",
    "            \n",
    "            print(f\"üìã {collection_name}:\")\n",
    "            print(f\"   üìÑ Documentos: {count:,}\")\n",
    "            print(f\"   üè∑Ô∏è Campos: {fields_count}\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"üìä TOTAL DE DOCUMENTOS: {total_documents:,}\")\n",
    "        \n",
    "        # Validaciones espec√≠ficas\n",
    "        print(\"\\\\nüîç VALIDACIONES ESPEC√çFICAS:\")\n",
    "        \n",
    "        # 1. Verificar que existen √≥rdenes con estado 'delivered'\n",
    "        delivered_orders = db.orders.count_documents({'order_status': 'delivered'})\n",
    "        print(f\"‚úÖ √ìrdenes entregadas: {delivered_orders:,}\")\n",
    "        \n",
    "        # 2. Verificar rangos de fechas\n",
    "        date_range = db.orders.aggregate([\n",
    "            {'$group': {\n",
    "                '_id': None,\n",
    "                'min_date': {'$min': '$order_purchase_timestamp'},\n",
    "                'max_date': {'$max': '$order_purchase_timestamp'}\n",
    "            }}\n",
    "        ])\n",
    "        \n",
    "        for doc in date_range:\n",
    "            print(f\"üìÖ Rango de fechas: {doc['min_date'].strftime('%Y-%m-%d')} a {doc['max_date'].strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # 3. Verificar integridad referencial b√°sica\n",
    "        order_count = db.orders.count_documents({})\n",
    "        payment_count = db.payments.count_documents({})\n",
    "        print(f\"üîó √ìrdenes: {order_count:,}, Pagos: {payment_count:,}\")\n",
    "        \n",
    "        logger.info(\"‚úÖ Validaci√≥n completada exitosamente\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error en validaci√≥n: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    finally:\n",
    "        if client:\n",
    "            client.close()\n",
    "\n",
    "def run_sample_queries():\n",
    "    \"\"\"\n",
    "    Ejecuta consultas de ejemplo para demostrar funcionalidad\n",
    "    \"\"\"\n",
    "    logger.info(\"üîç Ejecutando consultas de ejemplo...\")\n",
    "    \n",
    "    client, db = get_mongodb_connection()\n",
    "    \n",
    "    if db is None:\n",
    "        logger.error(\"‚ùå No se pudo conectar a MongoDB\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(\"üîç CONSULTAS DE EJEMPLO EN MONGODB\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # 1. Top 5 estados con m√°s clientes\n",
    "        print(\"1Ô∏è‚É£ Top 5 Estados con M√°s Clientes:\")\n",
    "        top_states = db.customers.aggregate([\n",
    "            {'$group': {'_id': '$customer_state', 'count': {'$sum': 1}}},\n",
    "            {'$sort': {'count': -1}},\n",
    "            {'$limit': 5}\n",
    "        ])\n",
    "        \n",
    "        for state in top_states:\n",
    "            print(f\"   {state['_id']}: {state['count']:,} clientes\")\n",
    "        \n",
    "        # 2. Promedio de satisfacci√≥n por mes\n",
    "        print(\"\\\\n2Ô∏è‚É£ Promedio de Satisfacci√≥n por Mes (√∫ltimos 6 meses):\")\n",
    "        monthly_satisfaction = db.reviews.aggregate([\n",
    "            {'$match': {'review_score': {'$exists': True}}},\n",
    "            {'$group': {\n",
    "                '_id': {\n",
    "                    'year': {'$year': '$review_creation_date'},\n",
    "                    'month': {'$month': '$review_creation_date'}\n",
    "                },\n",
    "                'avg_score': {'$avg': '$review_score'},\n",
    "                'count': {'$sum': 1}\n",
    "            }},\n",
    "            {'$sort': {'_id.year': -1, '_id.month': -1}},\n",
    "            {'$limit': 6}\n",
    "        ])\n",
    "        \n",
    "        for month in monthly_satisfaction:\n",
    "            year = month['_id']['year']\n",
    "            month_num = month['_id']['month']\n",
    "            avg_score = month['avg_score']\n",
    "            count = month['count']\n",
    "            print(f\"   {year}-{month_num:02d}: {avg_score:.2f} ‚≠ê ({count} reviews)\")\n",
    "        \n",
    "        # 3. Top 5 categor√≠as por revenue\n",
    "        print(\"\\\\n3Ô∏è‚É£ Top 5 Categor√≠as por Revenue:\")\n",
    "        category_revenue = db.order_items.aggregate([\n",
    "            {'$lookup': {\n",
    "                'from': 'products',\n",
    "                'localField': 'product_id',\n",
    "                'foreignField': 'product_id',\n",
    "                'as': 'product_info'\n",
    "            }},\n",
    "            {'$unwind': '$product_info'},\n",
    "            {'$group': {\n",
    "                '_id': '$product_info.product_category_name_english',\n",
    "                'total_revenue': {'$sum': '$price'},\n",
    "                'total_orders': {'$sum': 1}\n",
    "            }},\n",
    "            {'$sort': {'total_revenue': -1}},\n",
    "            {'$limit': 5}\n",
    "        ])\n",
    "        \n",
    "        for category in category_revenue:\n",
    "            name = category['_id'] or 'Unknown'\n",
    "            revenue = category['total_revenue']\n",
    "            orders = category['total_orders']\n",
    "            print(f\"   {name}: R$ {revenue:,.2f} ({orders:,} items)\")\n",
    "        \n",
    "        # 4. Distribuci√≥n de m√©todos de pago\n",
    "        print(\"\\\\n4Ô∏è‚É£ Distribuci√≥n de M√©todos de Pago:\")\n",
    "        payment_distribution = db.payments.aggregate([\n",
    "            {'$group': {\n",
    "                '_id': '$payment_type',\n",
    "                'count': {'$sum': 1},\n",
    "                'total_value': {'$sum': '$payment_value'}\n",
    "            }},\n",
    "            {'$sort': {'count': -1}}\n",
    "        ])\n",
    "        \n",
    "        for payment in payment_distribution:\n",
    "            method = payment['_id']\n",
    "            count = payment['count']\n",
    "            value = payment['total_value']\n",
    "            print(f\"   {method}: {count:,} transacciones (R$ {value:,.2f})\")\n",
    "        \n",
    "        logger.info(\"‚úÖ Consultas de ejemplo completadas\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error ejecutando consultas: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        if client:\n",
    "            client.close()\n",
    "\n",
    "# Funciones de utilidad para consultas\n",
    "print(\"üõ†Ô∏è FUNCIONES DE VALIDACI√ìN Y CONSULTAS LISTAS\")\n",
    "print(\"üìù Para usar:\")\n",
    "print(\"   - validate_mongodb_data() : Valida los datos cargados\")\n",
    "print(\"   - run_sample_queries() : Ejecuta consultas de ejemplo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03dab5",
   "metadata": {},
   "source": [
    "### ‚úÖ **Funciones de Validaci√≥n y Consultas de Ejemplo**\n",
    "\n",
    "Esta secci√≥n proporciona herramientas para validar y explorar los datos cargados en MongoDB:\n",
    "\n",
    "#### **üîç `validate_mongodb_data()` - Validaci√≥n de Integridad**\n",
    "\n",
    "Funci√≥n completa de validaci√≥n post-ETL que verifica:\n",
    "\n",
    "**üìä Estad√≠sticas b√°sicas:**\n",
    "- Conteo de documentos por colecci√≥n\n",
    "- N√∫mero de campos por tipo de documento\n",
    "- Total de documentos en la base de datos\n",
    "\n",
    "**‚úÖ Validaciones espec√≠ficas:**\n",
    "- **√ìrdenes entregadas**: Verifica que existen √≥rdenes con status 'delivered'\n",
    "- **Rangos de fechas**: Valida que las fechas est√°n en rangos esperados\n",
    "- **Integridad referencial**: Compara conteos entre colecciones relacionadas\n",
    "- **Consistencia de datos**: Verifica que no hay datos corruptos\n",
    "\n",
    "#### **üîç `run_sample_queries()` - Consultas Demostrativas**\n",
    "\n",
    "Ejecuta consultas representativas para demostrar la funcionalidad de MongoDB:\n",
    "\n",
    "**üìà Consultas implementadas:**\n",
    "\n",
    "1. **Top 5 Estados por Clientes**: An√°lisis geogr√°fico de la base de clientes\n",
    "2. **Satisfacci√≥n Temporal**: Promedio de satisfacci√≥n por mes (√∫ltimos 6 meses)\n",
    "3. **Revenue por Categor√≠a**: Top 5 categor√≠as de productos por ingresos\n",
    "4. **M√©todos de Pago**: Distribuci√≥n de tipos de pago y valores\n",
    "\n",
    "**üéØ Prop√≥sito de las consultas:**\n",
    "- **Validar joins**: Verificar que las relaciones entre colecciones funcionan\n",
    "- **Probar agregaciones**: Confirmar que MongoDB puede realizar c√°lculos complejos\n",
    "- **Demostrar capacidades**: Mostrar el tipo de an√°lisis posibles\n",
    "- **Benchmark**: Medir performance de consultas con √≠ndices\n",
    "\n",
    "**üìù Uso recomendado:**\n",
    "```python\n",
    "# Despu√©s del ETL, validar la carga\n",
    "validate_mongodb_data()\n",
    "\n",
    "# Explorar datos con consultas de ejemplo\n",
    "run_sample_queries()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0ee0e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:32:32,291 - INFO - üîç Validando datos en MongoDB...\n",
      "2025-08-13 22:32:32,315 - INFO - ‚úÖ Conexi√≥n exitosa a MongoDB: olist_ecommerce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ESTAD√çSTICAS DE DATOS EN MONGODB\n",
      "==================================================\n",
      "üìã customers:\n",
      "   üìÑ Documentos: 99,441\n",
      "   üè∑Ô∏è Campos: 7\n",
      "\n",
      "üìã orders:\n",
      "   üìÑ Documentos: 99,441\n",
      "   üè∑Ô∏è Campos: 16\n",
      "\n",
      "üìã products:\n",
      "   üìÑ Documentos: 32,951\n",
      "   üè∑Ô∏è Campos: 13\n",
      "\n",
      "üìã order_items:\n",
      "   üìÑ Documentos: 112,650\n",
      "   üè∑Ô∏è Campos: 11\n",
      "\n",
      "üìã payments:\n",
      "   üìÑ Documentos: 103,886\n",
      "   üè∑Ô∏è Campos: 7\n",
      "\n",
      "üìã reviews:\n",
      "   üìÑ Documentos: 99,224\n",
      "   üè∑Ô∏è Campos: 12\n",
      "\n",
      "üìã sellers:\n",
      "   üìÑ Documentos: 3,095\n",
      "   üè∑Ô∏è Campos: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:32:33,014 - INFO - ‚úÖ Validaci√≥n completada exitosamente\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã geolocation:\n",
      "   üìÑ Documentos: 1,000,163\n",
      "   üè∑Ô∏è Campos: 6\n",
      "\n",
      "üìä TOTAL DE DOCUMENTOS: 1,550,851\n",
      "\\nüîç VALIDACIONES ESPEC√çFICAS:\n",
      "‚úÖ √ìrdenes entregadas: 96,478\n",
      "üìÖ Rango de fechas: 2016-09-04 a 2018-10-17\n",
      "üîó √ìrdenes: 99,441, Pagos: 103,886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_mongodb_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f711d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 22:32:43,796 - INFO - üîç Ejecutando consultas de ejemplo...\n",
      "2025-08-13 22:32:43,807 - INFO - ‚úÖ Conexi√≥n exitosa a MongoDB: olist_ecommerce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CONSULTAS DE EJEMPLO EN MONGODB\n",
      "==================================================\n",
      "1Ô∏è‚É£ Top 5 Estados con M√°s Clientes:\n",
      "   SP: 41,746 clientes\n",
      "   RJ: 12,852 clientes\n",
      "   MG: 11,635 clientes\n",
      "   RS: 5,466 clientes\n",
      "   PR: 5,045 clientes\n",
      "\\n2Ô∏è‚É£ Promedio de Satisfacci√≥n por Mes (√∫ltimos 6 meses):\n",
      "   2018-08: 4.21 ‚≠ê (8987 reviews)\n",
      "   2018-07: 4.29 ‚≠ê (5634 reviews)\n",
      "   2018-06: 4.20 ‚≠ê (6715 reviews)\n",
      "   2018-05: 4.19 ‚≠ê (7458 reviews)\n",
      "   2018-04: 3.92 ‚≠ê (7287 reviews)\n",
      "   2018-03: 3.73 ‚≠ê (7803 reviews)\n",
      "\\n3Ô∏è‚É£ Top 5 Categor√≠as por Revenue:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_sample_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mrun_sample_queries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# 3. Top 5 categor√≠as por revenue\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn3Ô∏è‚É£ Top 5 Categor√≠as por Revenue:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m category_revenue = \u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43morder_items\u001b[49m\u001b[43m.\u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$lookup\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfrom\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproducts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocalField\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mforeignField\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mas\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_info\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$unwind\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$product_info\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$group\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$product_info.product_category_name_english\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_revenue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$sum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$price\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_orders\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$sum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$sort\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_revenue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$limit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m category_revenue:\n\u001b[32m    143\u001b[39m     name = category[\u001b[33m'\u001b[39m\u001b[33m_id\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mUnknown\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\collection.py:2979\u001b[39m, in \u001b[36mCollection.aggregate\u001b[39m\u001b[34m(self, pipeline, session, let, comment, **kwargs)\u001b[39m\n\u001b[32m   2902\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Perform an aggregation using the aggregation framework on this\u001b[39;00m\n\u001b[32m   2903\u001b[39m \u001b[33;03mcollection.\u001b[39;00m\n\u001b[32m   2904\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2976\u001b[39m \u001b[33;03m    https://mongodb.com/docs/manual/reference/command/aggregate\u001b[39;00m\n\u001b[32m   2977\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2978\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._database.client._tmp_session(session, close=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[32m-> \u001b[39m\u001b[32m2979\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_aggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_CollectionAggregationCommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCommandCursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexplicit_session\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2987\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2988\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\_csot.py:125\u001b[39m, in \u001b[36mapply.<locals>.csot_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\collection.py:2886\u001b[39m, in \u001b[36mCollection._aggregate\u001b[39m\u001b[34m(self, aggregation_command, pipeline, cursor_class, session, explicit_session, let, comment, **kwargs)\u001b[39m\n\u001b[32m   2875\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcomment\u001b[39m\u001b[33m\"\u001b[39m] = comment\n\u001b[32m   2876\u001b[39m cmd = aggregation_command(\n\u001b[32m   2877\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2878\u001b[39m     cursor_class,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2883\u001b[39m     user_fields={\u001b[33m\"\u001b[39m\u001b[33mcursor\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mfirstBatch\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m}},\n\u001b[32m   2884\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2886\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_database\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_retryable_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_cursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_read_preference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_performs_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2891\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_Op\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAGGREGATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2892\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:2026\u001b[39m, in \u001b[36mMongoClient._retryable_read\u001b[39m\u001b[34m(self, func, read_pref, session, operation, address, retryable, operation_id)\u001b[39m\n\u001b[32m   2021\u001b[39m \u001b[38;5;66;03m# Ensure that the client supports retrying on reads and there is no session in\u001b[39;00m\n\u001b[32m   2022\u001b[39m \u001b[38;5;66;03m# transaction, otherwise, we will not support retry behavior for this call.\u001b[39;00m\n\u001b[32m   2023\u001b[39m retryable = \u001b[38;5;28mbool\u001b[39m(\n\u001b[32m   2024\u001b[39m     retryable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.options.retry_reads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (session \u001b[38;5;129;01mand\u001b[39;00m session.in_transaction)\n\u001b[32m   2025\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_read\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m=\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\_csot.py:125\u001b[39m, in \u001b[36mapply.<locals>.csot_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:1993\u001b[39m, in \u001b[36mMongoClient._retry_internal\u001b[39m\u001b[34m(self, func, session, bulk, operation, is_read, address, read_pref, retryable, operation_id)\u001b[39m\n\u001b[32m   1956\u001b[39m \u001b[38;5;129m@_csot\u001b[39m.apply\n\u001b[32m   1957\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retry_internal\u001b[39m(\n\u001b[32m   1958\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1967\u001b[39m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1968\u001b[39m ) -> T:\n\u001b[32m   1969\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Internal retryable helper for all client transactions.\u001b[39;00m\n\u001b[32m   1970\u001b[39m \n\u001b[32m   1971\u001b[39m \u001b[33;03m    :param func: Callback function we want to retry\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1980\u001b[39m \u001b[33;03m    :return: Output of the calling func()\u001b[39;00m\n\u001b[32m   1981\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1982\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ClientConnectionRetryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmongo_client\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbulk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1986\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_read\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m=\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1992\u001b[39m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1993\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:2730\u001b[39m, in \u001b[36m_ClientConnectionRetryable.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2728\u001b[39m \u001b[38;5;28mself\u001b[39m._check_last_error(check_csot=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2729\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_read \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write()\n\u001b[32m   2731\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[32m   2732\u001b[39m     \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[32m   2733\u001b[39m     \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[32m   2734\u001b[39m     \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n\u001b[32m   2735\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_last_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:2891\u001b[39m, in \u001b[36m_ClientConnectionRetryable._read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrying:\n\u001b[32m   2884\u001b[39m     _debug_log(\n\u001b[32m   2885\u001b[39m         _COMMAND_LOGGER,\n\u001b[32m   2886\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrying read attempt number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._attempt_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2889\u001b[39m         operationId=\u001b[38;5;28mself\u001b[39m._operation_id,\n\u001b[32m   2890\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2891\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_server\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\aggregation.py:164\u001b[39m, in \u001b[36m_AggregationCommand.get_cursor\u001b[39m\u001b[34m(self, session, server, conn, read_preference)\u001b[39m\n\u001b[32m    161\u001b[39m     write_concern = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Run command.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m result = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_database\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_target\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_concern\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_database\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_user_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result_processor:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._result_processor(result, conn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\helpers.py:47\u001b[39m, in \u001b[36m_handle_reauth.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpymongo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msynchronous\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Connection\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailure \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m no_reauth:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\pool.py:442\u001b[39m, in \u001b[36mConnection.command\u001b[39m\u001b[34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Catch socket.error, KeyboardInterrupt, CancelledError, etc. and close ourselves.\u001b[39;00m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_connection_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\pool.py:414\u001b[39m, in \u001b[36mConnection.command\u001b[39m\u001b[34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[39m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_if_not_writable(unacknowledged)\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_mongos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowable_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlisteners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_bson_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression_ctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_op_msg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mop_msg_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43munacknowledged\u001b[49m\u001b[43m=\u001b[49m\u001b[43munacknowledged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexhaust_allowed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexhaust_allowed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (OperationFailure, NotPrimaryError):\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\synchronous\\network.py:198\u001b[39m, in \u001b[36mcommand\u001b[39m\u001b[34m(conn, dbname, spec, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields, exhaust_allowed, write_concern)\u001b[39m\n\u001b[32m    196\u001b[39m     response_doc: _DocumentOut = {\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m}\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     reply = \u001b[43mreceive_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     conn.more_to_come = reply.more_to_come\n\u001b[32m    200\u001b[39m     unpacked_docs = reply.unpack_response(\n\u001b[32m    201\u001b[39m         codec_options=codec_options, user_fields=user_fields\n\u001b[32m    202\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\network_layer.py:759\u001b[39m, in \u001b[36mreceive_message\u001b[39m\u001b[34m(conn, request_id, max_message_size)\u001b[39m\n\u001b[32m    757\u001b[39m         deadline = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    758\u001b[39m \u001b[38;5;66;03m# Ignore the response's request id.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m length, _, response_to, op_code = _UNPACK_HEADER(\u001b[43mreceive_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# No request_id for exhaust cursor \"getMore\".\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\network_layer.py:343\u001b[39m, in \u001b[36mreceive_data\u001b[39m\u001b[34m(conn, length, deadline)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    340\u001b[39m     \u001b[38;5;66;03m# Use the legacy wait_for_read cancellation approach on PyPy due to PYTHON-5011.\u001b[39;00m\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# also use it on Windows due to PYTHON-5405\u001b[39;00m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _PYPY \u001b[38;5;129;01mor\u001b[39;00m _WINDOWS:\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m         \u001b[43mwait_for_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _csot.get_timeout() \u001b[38;5;129;01mand\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    345\u001b[39m             conn.set_conn_timeout(\u001b[38;5;28mmax\u001b[39m(deadline - time.monotonic(), \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\network_layer.py:316\u001b[39m, in \u001b[36mwait_for_read\u001b[39m\u001b[34m(conn, deadline)\u001b[39m\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    315\u001b[39m         timeout = _POLL_TIMEOUT\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     readable = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msocket_checker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.cancel_context.cancelled:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _OperationCancelled(\u001b[33m\"\u001b[39m\u001b[33moperation cancelled\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josue\\Documents\\sexto\\BDA\\Replicaci-n-Mongo\\env\\Lib\\site-packages\\pymongo\\socket_checker.py:77\u001b[39m, in \u001b[36mSocketChecker.select\u001b[39m\u001b[34m(self, sock, read, write, timeout)\u001b[39m\n\u001b[32m     75\u001b[39m rlist = [sock] \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m     76\u001b[39m wlist = [sock] \u001b[38;5;28;01mif\u001b[39;00m write \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m res = \u001b[43mselect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43msock\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# select returns a 3-tuple of lists of objects that are\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# ready: subsets of the first three arguments. Return\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# True if any of the lists are not empty.\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(res)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_sample_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be232dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
